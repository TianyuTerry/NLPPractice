{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import copy\n",
    "import itertools\n",
    "partial = Path('./data/partial')\n",
    "full = Path('./data/full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelled(path):\n",
    "    with open(path) as f:  \n",
    "        X, Y, x, y = list(), list(), list(), list()\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "                x, y = list(), list()\n",
    "            else:\n",
    "                word, tag = line.strip().split()\n",
    "                x.append(word)\n",
    "                y.append(tag)\n",
    "    return X, Y\n",
    "\n",
    "def unlabelled(path):\n",
    "    with open(path) as f:  \n",
    "        X, x = list(), list()\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                X.append(x)\n",
    "                x = list()\n",
    "            else:\n",
    "                word = line.strip()\n",
    "                x.append(word)\n",
    "    return X\n",
    "\n",
    "def read_data(root):\n",
    "    train, devin, devout = root/'train', root/'dev.in', root/'dev.out'     \n",
    "    return labelled(train), unlabelled(devin), labelled(devout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, devin_ds, devout_ds = read_data(partial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission_weight_smooth(train_ds, k):\n",
    "    \n",
    "    vocabulary = list(set([word for sentence in train_ds[0] for word in sentence]))\n",
    "    tags = list(set([tag for tags in train_ds[1] for tag in tags]))\n",
    "    word2index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    tag2index = {tag: i for i, tag in enumerate(tags)}\n",
    "    count_table = np.zeros((len(tags), len(vocabulary)))\n",
    "    for X, Y in zip(train_ds[0], train_ds[1]):\n",
    "        for word, tag in zip(X, Y):\n",
    "            count_table[tag2index[tag], word2index[word]] += 1\n",
    "    \n",
    "    removed_index = np.sum(count_table, 0) < k\n",
    "    \n",
    "    print('Number of removed words:', np.sum(removed_index))\n",
    "    print('Total number of words:', len(vocabulary))\n",
    "    \n",
    "    if (np.sum(removed_index) > 0):\n",
    "        \n",
    "        count_table = np.append(count_table, np.sum(count_table[:, removed_index], 1)[:,None], 1)\n",
    "        count_table = np.delete(count_table, np.nonzero(removed_index), 1)\n",
    "        \n",
    "        new_vocab = [vocabulary[j] for j in range(len(vocabulary)) if not removed_index[j]]+['#UNK#']\n",
    "        word2index = {w:i for i,w in enumerate(new_vocab)}\n",
    "    \n",
    "    count_table/=count_table.sum(1)[:, None]\n",
    "    \n",
    "    emission_weight = np.ma.log(count_table).filled(-np.inf)\n",
    "    \n",
    "    return emission_weight, word2index, tag2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed words: 2528\n",
      "Total number of words: 4068\n"
     ]
    }
   ],
   "source": [
    "emission_weight, word2index, tag2index = emission_weight_smooth(train_ds, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = train_ds[0], train_ds[1]\n",
    "test_X, test_Y = devout_ds[0], devout_ds[1]\n",
    "def tokenize(sentence):\n",
    "    return [word2index[word] if word in word2index else word2index['#UNK#'] for word in sentence]\n",
    "def tag2idx(tags):\n",
    "    return [tag2index[tag] for tag in tags]\n",
    "train_X = [tokenize(sentence) for sentence in train_X]\n",
    "test_X = [tokenize(sentence) for sentence in test_X]\n",
    "train_Y = [tag2idx(tags) for tags in train_Y]\n",
    "test_Y = [tag2idx(tags) for tags in test_Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_weight(train_ds, tag2index):\n",
    "    \n",
    "    T = len(tag2index)\n",
    "    count_table = np.zeros((T+1, T+1))\n",
    "    for Y in train_ds[1]:\n",
    "        count_table[-1, tag2index[Y[0]]] += 1\n",
    "        for i in range(len(Y)-1):\n",
    "            count_table[tag2index[Y[i]], tag2index[Y[i+1]]] += 1\n",
    "        count_table[tag2index[Y[-1]], -1] += 1\n",
    "            \n",
    "    count_table/=count_table.sum(1)[:, None]\n",
    "    \n",
    "    transition_weight = np.ma.log(count_table).filled(-np.inf)\n",
    "    \n",
    "    return transition_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_weight = transition_weight(train_ds, tag2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi and Evaluation for HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(X, tag2index, emission_weight, transition_weight):\n",
    "    \n",
    "    index2tag = {value: key for key, value in tag2index.items()}\n",
    "    score_matrix = np.zeros((len(tag2index), len(X)))\n",
    "    path_matrix = np.zeros((len(tag2index), len(X)), dtype='int')\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        if i == 0:\n",
    "            score_matrix[:, i] = transition_weight[-1, :-1] + emission_weight[:, X[i]]\n",
    "        else:\n",
    "            for j in range(len(tag2index)):\n",
    "                competitors = emission_weight[j, X[i]] + transition_weight[:-1, j] + score_matrix[:, i-1]\n",
    "                score_matrix[j, i] = np.max(competitors)\n",
    "                path_matrix[j, i] = np.argmax(competitors)\n",
    "    \n",
    "    competitors = transition_weight[:-1, -1] + score_matrix[:, -1]\n",
    "    last_idx = np.argmax(competitors)\n",
    "    path = [last_idx]\n",
    "    for m in range(len(X)-1, 0, -1):\n",
    "        path.insert(0, path_matrix[path[0], m])\n",
    "    output_tags = [index2tag[idx] for idx in path]\n",
    "    return output_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_output(dev_out_path, X_raw, tag2index, emission_weight, transition_weight):\n",
    "    \n",
    "    X = [tokenize(sentence) for sentence in X_raw]\n",
    "    tags = [viterbi(sentence, tag2index, emission_weight, transition_weight) for sentence in X]\n",
    "    \n",
    "    output_string = ''\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[i])):\n",
    "            output_string += X_raw[i][j] + ' ' + tags[i][j] + '\\n'\n",
    "        output_string += '\\n'\n",
    "    \n",
    "    with open(dev_out_path, 'w') as f:\n",
    "        f.write(output_string)\n",
    "    \n",
    "    print('Done with writing predictions')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing predictions\n"
     ]
    }
   ],
   "source": [
    "dev_out_path = partial/'dev.p2.out'\n",
    "viterbi_output(dev_out_path, devin_ds[0], tag2index, emission_weight, transition_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conlleval_ import evaluate\n",
    "\n",
    "def sequence_evaluation(X, Y, tag2index, emission_dict, transition_dict):\n",
    "    index2tag = {value: key for key, value in tag2index.items()}\n",
    "    tags_ = [viterbi(sentence, tag2index, emission_dict, transition_dict) for sentence in X]\n",
    "    tags = [tag for tags in tags_ for tag in tags]\n",
    "    Y  = [tag for tags in Y for tag in tags]\n",
    "    Y = [index2tag[idx] for idx in Y]\n",
    "    assert len(Y) == len(tags)\n",
    "    return evaluate(Y, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 2097 tokens with 236 phrases; found: 187 phrases; correct: 115.\n",
      "accuracy:  54.12%; (non-O)\n",
      "accuracy:  89.99%; precision:  61.50%; recall:  48.73%; FB1:  54.37\n",
      "              art: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "              eve: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "              geo: precision:  55.13%; recall:  50.59%; FB1:  52.76  78\n",
      "              gpe: precision:  94.12%; recall:  64.00%; FB1:  76.19  17\n",
      "              nat: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "              org: precision:  41.67%; recall:  28.57%; FB1:  33.90  24\n",
      "              per: precision:  50.00%; recall:  37.50%; FB1:  42.86  24\n",
      "              tim: precision:  77.27%; recall:  64.15%; FB1:  70.10  44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61.49732620320856, 48.728813559322035, 54.37352245862884)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_evaluation(test_X, test_Y, tag2index, emission_weight, transition_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss & Forward & Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, tag2index, emission_weight, transition_weight):\n",
    "    \"\"\"\n",
    "    The returned value should be the forward matrix for sentence X and the final alpha\n",
    "    The implementation so far explicitly pass emission and transition dictionary as arguments\n",
    "    This can be replaced by FeatureSum function in the implementation for part 5\n",
    "    \"\"\"\n",
    "    forward_matrix = np.zeros((len(tag2index), len(X)))\n",
    "    emission_weight_exp = np.exp(emission_weight)\n",
    "    transition_weight_exp = np.exp(transition_weight)\n",
    "    for i in range(len(X)):\n",
    "        if i == 0:\n",
    "            forward_matrix[:, i] = transition_weight_exp[-1, :-1]*emission_weight_exp[:, X[i]]\n",
    "        else:\n",
    "            for j in range(len(tag2index)):\n",
    "                SumPotential = np.sum(emission_weight_exp[j, X[i]]*transition_weight_exp[:-1, j]*forward_matrix[:, i-1])\n",
    "                forward_matrix[j, i] = SumPotential\n",
    "    \n",
    "    SumPotential = np.sum(transition_weight_exp[:-1, -1]*forward_matrix[:, -1])\n",
    "    \n",
    "    return forward_matrix, SumPotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, tag2index, emission_weight, transition_weight):\n",
    "    \"\"\"\n",
    "    The returned value should be the forward matrix for sentence X and the final beta\n",
    "    final beta should be the same as final alpha\n",
    "    This is considered correct according to the definition of backward algorithm\n",
    "    \"\"\"\n",
    "    backward_matrix = np.zeros((len(tag2index), len(X)))\n",
    "    emission_weight_exp = np.exp(emission_weight)\n",
    "    transition_weight_exp = np.exp(transition_weight)\n",
    "    \n",
    "    \n",
    "    backward_matrix[:, -1] = transition_weight_exp[:-1, -1]\n",
    "    \n",
    "    for i in range(len(X)-2, -1, -1):\n",
    "        for j in range(len(tag2index)):\n",
    "            SumPotential = np.sum(transition_weight_exp[j, :-1]*emission_weight_exp[:, X[i+1]]*backward_matrix[:, i+1])\n",
    "            backward_matrix[j, i] = SumPotential\n",
    "    \n",
    "    SumPotential = np.sum(transition_weight_exp[-1, :-1]*emission_weight_exp[:, X[0]]*backward_matrix[:, 0])\n",
    "    \n",
    "    return backward_matrix, SumPotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(X, Y, tag2index, emission_weight, transition_weight):\n",
    "    \n",
    "    pair_score = 0\n",
    "    for i in range(len(X)):\n",
    "        if i == 0:\n",
    "            emission_score = emission_weight[Y[i], X[i]]\n",
    "            transition_score = transition_weight[-1, Y[i]]\n",
    "            pair_score += (transition_score+emission_score)\n",
    "        else:\n",
    "            emission_score = emission_weight[Y[i], X[i]]\n",
    "            transition_score = transition_weight[Y[i-1], Y[i]]\n",
    "            pair_score += (transition_score+emission_score)\n",
    "    \n",
    "    transition_score = transition_weight[Y[-1], -1]\n",
    "    pair_score += transition_score\n",
    "    \n",
    "    _, SumPotential = forward(X, tag2index, emission_weight, transition_weight)\n",
    "    \n",
    "    return -(pair_score-np.log(SumPotential))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossDataset(Xs, Ys, tag2index, emission_weight, transition_weight):\n",
    "    return np.sum([Loss(X, Y, tag2index, emission_weight, transition_weight) for X, Y in zip(Xs, Ys)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156.7293881993182"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LossDataset(train_X, train_Y, tag2index, emission_weight, transition_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.12443499400999e-50, 1.12443499400999e-50)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, forward_sum = forward(train_X[0], tag2index, emission_weight, transition_weight)\n",
    "_, backward_sum = backward(train_X[0], tag2index, emission_weight, transition_weight)\n",
    "forward_sum, backward_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Count & Empirical Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_count_emission(X, tag, word, tag2index, word2index, emission_weight, transition_weight):\n",
    "    \"\"\"\n",
    "    This is considered correct according to the definition of f(y_i-1, y_i, x_i)\n",
    "    tag is index and word is also index\n",
    "    \"\"\"\n",
    "    forward_matrix, NormalizationTerm = forward(X, tag2index, emission_weight, transition_weight)\n",
    "    backward_matrix, _ = backward(X, tag2index, emission_weight, transition_weight)\n",
    "    SumPotential = 0\n",
    "    \n",
    "    emission_score = np.exp(emission_weight[tag, word])\n",
    "    if X[0] == word:\n",
    "        print('index at which the word appears:', 0)\n",
    "        transition_score = np.exp(transition_weight[-1, tag])\n",
    "        SumPotential += backward_matrix[tag, 0]*emission_score*transition_score\n",
    "    for i in range(1, len(X)):\n",
    "        if X[i] == word:\n",
    "            print('index at which the word appears:', i)\n",
    "            transition_scores = np.exp(transition_weight[:-1, tag])\n",
    "            SumPotential += np.sum(forward_matrix[:, i-1]*backward_matrix[tag, i]*emission_score*transition_scores)\n",
    "    return SumPotential/NormalizationTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index at which the word appears: 1\n",
      "index at which the word appears: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9091088758591536"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_count_emission(train_X[1], tag2index['I-per'], \n",
    "                        word2index['#UNK#'], tag2index, word2index, \n",
    "                        emission_weight, transition_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_count_emission(X, Y, tag, word, tag2index, word2index):  \n",
    "    count = 0\n",
    "    for x, y in zip(X, Y):\n",
    "        if x == word and y == tag:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_count_emission(train_X[1], train_Y[1], tag2index['O'], word2index['#UNK#'], tag2index, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_count_transition(X, tag1, tag2, tag2index, word2index, emission_weight, transition_weight):\n",
    "    forward_matrix, NormalizationTerm = forward(X, tag2index, emission_weight, transition_weight)\n",
    "    backward_matrix, _ = backward(X, tag2index, emission_weight, transition_weight)\n",
    "    SumPotential = 0\n",
    "    \n",
    "    transition_score = np.exp(transition_weight[tag1, tag2])\n",
    "    if tag1 == -1:\n",
    "        emission_score = np.exp(emission_weight[tag2, X[0]])\n",
    "        SumPotential += transition_score*emission_score*backward_matrix[tag2, 0]\n",
    "    elif tag2 == -1:\n",
    "        SumPotential += forward_matrix[tag1, -1]*transition_score\n",
    "    else:\n",
    "        for i in range(len(X)-1):\n",
    "            emission_score = np.exp(emission_weight[tag2, X[i+1]])\n",
    "            SumPotential += forward_matrix[tag1, i]*transition_score*emission_score*backward_matrix[tag2, i+1]\n",
    "    return SumPotential/NormalizationTerm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.783140987648915"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_count_transition(train_X[1], tag2index['O'], tag2index['O'], \n",
    "                          tag2index, word2index, \n",
    "                          emission_weight, transition_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_count_transition(X, Y, tag1, tag2, tag2index, word2index):\n",
    "    count = 0\n",
    "    if tag1 == -1 and Y[0] == tag2:\n",
    "        count += 1\n",
    "    elif tag2 == -1 and Y[-1] == tag1:\n",
    "        count += 1\n",
    "    else:\n",
    "        for i in range(len(Y)-1):\n",
    "            if tag1 == Y[i] and tag2 == Y[i+1]:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientEmission(Xs, Ys, tag, word, tag2index, word2index, emission_weight, transition_weight):\n",
    "    with HiddenPrints():\n",
    "        expected_count = np.sum([expected_count_emission(X, tag, word, tag2index, word2index, emission_weight, transition_weight) for X in Xs])\n",
    "        actual_count = np.sum([actual_count_emission(X, Y, tag, word, tag2index, word2index) for X, Y in zip(Xs, Ys)])\n",
    "    return expected_count - actual_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientTransition(Xs, Ys, tag1, tag2, tag2index, word2index, emission_weight, transition_weight):\n",
    "    with HiddenPrints():\n",
    "        expected_count = np.sum([expected_count_transition(X, tag1, tag2, tag2index, word2index, emission_weight, transition_weight) for X in Xs])\n",
    "        actual_count = np.sum([actual_count_transition(X, Y, tag1, tag2, tag2index, word2index) for X, Y in zip(Xs, Ys)])\n",
    "    return expected_count - actual_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_emission(epsilon, X_test, Y_test, tag, word, tag2index, word2index, emission_weight, transition_weight):\n",
    "    emission_weight_copy = copy.deepcopy(emission_weight)\n",
    "    emission_weight_copy[tag, word] += epsilon\n",
    "    old_loss = Loss(X_test, Y_test, tag2index, emission_weight, transition_weight)\n",
    "    new_loss = Loss(X_test, Y_test, tag2index, emission_weight_copy, transition_weight)\n",
    "    expected_count = expected_count_emission(X_test, tag, word, tag2index, word2index, emission_weight, transition_weight)\n",
    "    actual_count = actual_count_emission(X_test, Y_test, tag, word, tag2index, word2index)\n",
    "    gradient = expected_count - actual_count\n",
    "    print('Actual loss change: {}, Change according to gradient: {}'.format(new_loss - old_loss, gradient*epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index at which the word appears: 1\n",
      "index at which the word appears: 9\n",
      "Actual loss change: -6.02587614650929e-06, Change according to gradient: -6.026719482022292e-06\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test = train_X[1], train_Y[1]\n",
    "epsilon = 0.0001\n",
    "test_emission(epsilon, X_test, Y_test, tag2index['O'], word2index['#UNK#'], \n",
    "              tag2index, word2index, emission_weight, transition_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transition(epsilon, X_test, Y_test, tag1, tag2, tag2index, word2index, emission_weight, transition_weight):\n",
    "    transition_weight_copy = copy.deepcopy(transition_weight)\n",
    "    transition_weight_copy[tag1, tag2] += epsilon\n",
    "    old_loss = Loss(X_test, Y_test, tag2index, emission_weight, transition_weight)\n",
    "    new_loss = Loss(X_test, Y_test, tag2index, emission_weight, transition_weight_copy)\n",
    "    expected_count = expected_count_transition(X_test, tag1, tag2, tag2index, word2index, emission_weight, transition_weight)\n",
    "    actual_count = actual_count_transition(X_test, Y_test, tag1, tag2, tag2index, word2index)\n",
    "    gradient = expected_count - actual_count\n",
    "    print('Actual loss change: {}, Change according to gradient: {}'.format(new_loss - old_loss, gradient*epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss change: -0.0003452907908894076, Change according to gradient: -0.00034531940321648556\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.0001\n",
    "X_test, Y_test = train_X[0], train_Y[0]\n",
    "test_transition(epsilon, X_test, Y_test, tag2index['O'], tag2index['O'], \n",
    "                tag2index, word2index, emission_weight, transition_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossDatasetRegularization(Xs, Ys, tag2index, emission_weight, transition_weight, param):   \n",
    "    return LossDataset(Xs, Ys, tag2index, emission_weight, transition_weight) +\\\n",
    "        param*(np.sum(emission_weight[emission_weight != -np.inf]**2) +\\\n",
    "               np.sum(transition_weight[transition_weight != -np.inf]**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12066.057801496318"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Regularization = 0.1\n",
    "LossDatasetRegularization(train_X, train_Y, tag2index, emission_weight, transition_weight, Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientTransitionAll(Xs, Ys, tag2index, word2index, emission_weight, transition_weight):\n",
    "    i = 1\n",
    "    T = len(tag2index)\n",
    "    gradient = np.zeros((T+1, T+1))\n",
    "    for tag1, tag2 in itertools.product(range(-1, T), range(-1, T)):\n",
    "        gradient[tag1, tag2] = GradientTransition(Xs, Ys, tag1, tag2, \n",
    "                                                  tag2index, word2index, emission_weight, transition_weight)\n",
    "        if i%10 == 0:\n",
    "            print('done with the {}th gradient'.format(i))\n",
    "        i += 1\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientEmissionAll(Xs, Ys, tag2index, word2index, emission_weight, transition_weight):\n",
    "    i = 1\n",
    "    T = len(tag2index)\n",
    "    V = len(word2index)\n",
    "    gradient = np.zeros((T, V))\n",
    "    for tag, word in itertools.product(range(T), range(V)):\n",
    "        gradient[tag, word] = GradientEmission(Xs, Ys, tag, word, tag2index, word2index, \n",
    "                                               emission_weight, transition_weight)\n",
    "        if i%10 == 0:\n",
    "            print('done with the {}th gradient'.format(i))\n",
    "        i += 1\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with the 10th gradient\n",
      "done with the 20th gradient\n",
      "done with the 30th gradient\n",
      "done with the 40th gradient\n",
      "done with the 50th gradient\n",
      "done with the 60th gradient\n",
      "done with the 70th gradient\n",
      "done with the 80th gradient\n",
      "done with the 90th gradient\n",
      "done with the 100th gradient\n",
      "done with the 110th gradient\n",
      "done with the 120th gradient\n",
      "done with the 130th gradient\n",
      "done with the 140th gradient\n",
      "done with the 150th gradient\n",
      "done with the 160th gradient\n",
      "done with the 170th gradient\n",
      "done with the 180th gradient\n",
      "done with the 190th gradient\n",
      "done with the 200th gradient\n",
      "done with the 210th gradient\n",
      "done with the 220th gradient\n",
      "done with the 230th gradient\n",
      "done with the 240th gradient\n",
      "done with the 250th gradient\n",
      "done with the 260th gradient\n",
      "done with the 270th gradient\n",
      "done with the 280th gradient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1072.772547006607"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "gradient_transition = GradientTransitionAll(train_X, train_Y, tag2index, word2index, \n",
    "                                            emission_weight, transition_weight)\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make use of forward backward to calculate all features (emission and transition separately) all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_count_transition_all(Xs, tag2index, word2index, emission_weight, transition_weight):\n",
    "    \n",
    "    T = len(tag2index)\n",
    "    counter = 1\n",
    "    Expected_count = np.zeros((T+1, T+1))\n",
    "    \n",
    "    for X in Xs:\n",
    "        forward_matrix, NormalizationTerm = forward(X, tag2index, emission_weight, transition_weight)\n",
    "        backward_matrix, _ = backward(X, tag2index, emission_weight, transition_weight)\n",
    "\n",
    "        expected_count = np.zeros((T+1, T+1))\n",
    "        for tag1, tag2 in itertools.product(range(-1, T), range(-1, T)):\n",
    "            SumPotential = 0\n",
    "\n",
    "            transition_score = np.exp(transition_weight[tag1, tag2])\n",
    "            if tag1 == -1:\n",
    "                emission_score = np.exp(emission_weight[tag2, X[0]])\n",
    "                SumPotential += transition_score*emission_score*backward_matrix[tag2, 0]\n",
    "            elif tag2 == -1:\n",
    "                SumPotential += forward_matrix[tag1, -1]*transition_score\n",
    "            else:\n",
    "                for i in range(len(X)-1):\n",
    "                    emission_score = np.exp(emission_weight[tag2, X[i+1]])\n",
    "                    SumPotential += forward_matrix[tag1, i]*transition_score*emission_score*backward_matrix[tag2, i+1]\n",
    "            expected_count[tag1, tag2] = SumPotential/NormalizationTerm\n",
    "        Expected_count += expected_count\n",
    "        if counter%100 == 0:\n",
    "            print('Transition: done with the {}th instances'.format(counter))\n",
    "        counter += 1\n",
    "    return Expected_count\n",
    "        \n",
    "def actual_count_transition_all(Xs, Ys, tag2index, word2index):\n",
    "    T = len(tag2index)\n",
    "    Empirical_count = np.zeros((T+1, T+1))\n",
    "    for X, Y in zip(Xs, Ys):\n",
    "        empirical_count = np.zeros((T+1, T+1))\n",
    "        for tag1, tag2 in itertools.product(range(-1, T), range(-1, T)):\n",
    "            empirical_count[tag1, tag2] = actual_count_transition(X, Y, tag1, tag2, tag2index, word2index)\n",
    "        Empirical_count += empirical_count\n",
    "    return Empirical_count\n",
    "\n",
    "def GradientTransitionAllFast(Xs, Ys, tag2index, word2index, emission_weight, transition_weight):\n",
    "    return expected_count_transition_all(Xs, tag2index, word2index, emission_weight, transition_weight) -\\\n",
    "            actual_count_transition_all(Xs, Ys, tag2index, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition: done with the 100th instances\n",
      "Transition: done with the 200th instances\n",
      "Transition: done with the 300th instances\n",
      "Transition: done with the 400th instances\n",
      "Transition: done with the 500th instances\n",
      "Transition: done with the 600th instances\n",
      "Transition: done with the 700th instances\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.653486013412476"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "gradient_transition_fast = GradientTransitionAllFast(train_X, train_Y, tag2index, word2index, \n",
    "                                            emission_weight, transition_weight)\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare two methods for gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.912958928571243e-12"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(gradient_transition_fast - gradient_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_count_emission_all(Xs, tag2index, word2index, emission_weight, transition_weight):\n",
    "    \n",
    "    T = len(tag2index)\n",
    "    V = len(word2index)\n",
    "    counter = 1\n",
    "    Expected_count = np.zeros((T, V))\n",
    "    \n",
    "    for X in Xs:\n",
    "        forward_matrix, NormalizationTerm = forward(X, tag2index, emission_weight, transition_weight)\n",
    "        backward_matrix, _ = backward(X, tag2index, emission_weight, transition_weight)\n",
    "    \n",
    "        expected_count = np.zeros((T, V))\n",
    "        for tag, word in itertools.product(range(T), range(V)):\n",
    "            SumPotential = 0\n",
    "        \n",
    "            emission_score = np.exp(emission_weight[tag, word])\n",
    "            if X[0] == word:\n",
    "                transition_score = np.exp(transition_weight[-1, tag])\n",
    "                SumPotential += backward_matrix[tag, 0]*emission_score*transition_score\n",
    "            for i in range(1, len(X)):\n",
    "                if X[i] == word:\n",
    "                    transition_scores = np.exp(transition_weight[:-1, tag])\n",
    "                    SumPotential += np.sum(forward_matrix[:, i-1]*backward_matrix[tag, i]*emission_score*transition_scores)\n",
    "            \n",
    "            expected_count[tag, word] = SumPotential/NormalizationTerm\n",
    "    \n",
    "        Expected_count += expected_count\n",
    "        if counter%100 == 0:\n",
    "            print('Emission: done with the {}th instances'.format(counter))\n",
    "        counter += 1\n",
    "    return Expected_count\n",
    "\n",
    "def actual_count_emission_all(Xs, Ys, tag2index, word2index):\n",
    "    T = len(tag2index)\n",
    "    V = len(word2index)\n",
    "    Empirical_count = np.zeros((T, V))\n",
    "    for X, Y in zip(Xs, Ys):\n",
    "        empirical_count = np.zeros((T, V))\n",
    "        for tag, word in itertools.product(range(T), range(V)):\n",
    "            empirical_count[tag, word] = actual_count_emission(X, Y, tag, word, tag2index, word2index)\n",
    "        Empirical_count += empirical_count\n",
    "    return Empirical_count\n",
    "\n",
    "def GradientEmissionAllFast(Xs, Ys, tag2index, word2index, emission_weight, transition_weight):\n",
    "    return expected_count_emission_all(Xs, tag2index, word2index, emission_weight, transition_weight) -\\\n",
    "            actual_count_emission_all(Xs, Ys, tag2index, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with the 100th instances\n",
      "done with the 200th instances\n",
      "done with the 300th instances\n",
      "done with the 400th instances\n",
      "done with the 500th instances\n",
      "done with the 600th instances\n",
      "done with the 700th instances\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.55844497680664"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "gradient_emission_fast = GradientEmissionAllFast(train_X, train_Y, tag2index, word2index, \n",
    "                                            emission_weight, transition_weight)\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientTransitionAllFastRegularization(Xs, Ys, tag2index, word2index, emission_weight, transition_weight, param):\n",
    "    return expected_count_transition_all(Xs, tag2index, word2index, emission_weight, transition_weight) -\\\n",
    "            actual_count_transition_all(Xs, Ys, tag2index, word2index) +\\\n",
    "            2*param*transition_weight\n",
    "\n",
    "def GradientEmissionAllFastRegularization(Xs, Ys, tag2index, word2index, emission_weight, transition_weight, param):\n",
    "    return expected_count_emission_all(Xs, tag2index, word2index, emission_weight, transition_weight) -\\\n",
    "            actual_count_emission_all(Xs, Ys, tag2index, word2index) +\\\n",
    "            2*param*emission_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization for transition and emission features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:18660.6024\n",
      "Loss:14330.6897\n",
      "Loss:13133.8326\n",
      "Loss:12688.4027\n",
      "Loss:12240.1333\n",
      "Loss:10976.3865\n",
      "Loss:10278.0715\n",
      "Loss:9360.2376\n",
      "Loss:8857.4770\n",
      "Loss:8058.7878\n",
      "Loss:7588.8933\n",
      "Loss:7307.3267\n",
      "Loss:7102.4883\n",
      "Loss:6987.0037\n",
      "Loss:6775.7142\n",
      "Loss:6582.0261\n",
      "Loss:6348.0543\n",
      "Loss:6145.4270\n",
      "Loss:5935.1744\n",
      "Loss:5586.2842\n",
      "Loss:5416.3661\n",
      "Loss:5256.6581\n",
      "Loss:5123.4694\n",
      "Loss:5061.2558\n",
      "Loss:4977.0529\n",
      "Loss:4839.2782\n",
      "Loss:4784.0254\n",
      "Loss:4734.2525\n",
      "Loss:4622.3720\n",
      "Loss:4463.0371\n",
      "Loss:4323.5630\n",
      "Loss:4168.4023\n",
      "Loss:4097.3472\n",
      "Loss:4050.5129\n",
      "Loss:4004.9653\n",
      "Loss:3952.8393\n",
      "Loss:3884.9582\n",
      "Loss:3850.4644\n",
      "Loss:3802.7858\n",
      "Loss:3767.4955\n",
      "Loss:3735.7016\n",
      "Loss:3683.9396\n",
      "Loss:3645.8149\n",
      "Loss:3629.8610\n",
      "Loss:3612.8606\n",
      "Loss:3587.9989\n",
      "Loss:3571.6220\n",
      "Loss:3526.7502\n",
      "Loss:3495.6895\n",
      "Loss:3470.2333\n",
      "Loss:3454.3672\n",
      "Loss:3429.2142\n",
      "Loss:3410.1779\n",
      "Loss:3399.8245\n",
      "Loss:3373.5858\n",
      "Loss:3363.6444\n",
      "Loss:3353.8590\n",
      "Loss:3332.3631\n",
      "Loss:3322.0590\n",
      "Loss:3310.8206\n",
      "Loss:3305.1944\n",
      "Loss:3298.2365\n",
      "Loss:3286.7607\n",
      "Loss:3282.0531\n",
      "Loss:3268.5851\n",
      "Loss:3260.2488\n",
      "Loss:3255.6562\n",
      "Loss:3252.3171\n",
      "Loss:3248.3172\n",
      "Loss:3244.4967\n",
      "Loss:3238.9390\n",
      "Loss:3230.2516\n",
      "Loss:3221.7107\n",
      "Loss:3218.2531\n",
      "Loss:3214.3318\n",
      "Loss:3209.9126\n",
      "Loss:3206.8518\n",
      "Loss:3204.6197\n",
      "Loss:3200.7702\n",
      "Loss:3199.4680\n",
      "Loss:3197.6417\n",
      "Loss:3194.6741\n",
      "Loss:3191.5377\n",
      "Loss:3188.1687\n",
      "Loss:3185.6577\n",
      "Loss:3183.7239\n",
      "Loss:3182.4619\n",
      "Loss:3180.5823\n",
      "Loss:3178.2728\n",
      "Loss:3177.8812\n",
      "Loss:3176.3799\n",
      "Loss:3175.5807\n",
      "Loss:3175.0434\n",
      "Loss:3174.5435\n",
      "Loss:3173.0471\n",
      "Loss:3171.1147\n",
      "Loss:3170.9556\n",
      "Loss:3169.6676\n",
      "Loss:3169.0469\n",
      "Loss:3168.4929\n",
      "Loss:3167.9857\n",
      "Loss:3167.6634\n",
      "Loss:3167.0867\n",
      "Loss:3166.7090\n",
      "Loss:3165.8578\n",
      "Loss:3165.3225\n",
      "Loss:3164.5959\n",
      "Loss:3164.1615\n",
      "Loss:3164.0365\n",
      "Loss:3163.7298\n",
      "Loss:3163.6226\n",
      "Loss:3163.3626\n",
      "Loss:3163.1279\n",
      "Loss:3162.9724\n",
      "Loss:3162.7107\n",
      "Loss:3162.5826\n",
      "Loss:3162.2044\n",
      "Loss:3161.9248\n",
      "Loss:3161.6424\n",
      "Loss:3161.4219\n",
      "Loss:3161.2424\n",
      "Loss:3161.0848\n",
      "Loss:3160.7908\n",
      "Loss:3160.5520\n",
      "Loss:3160.4075\n",
      "Loss:3160.3384\n",
      "Loss:3160.2636\n",
      "Loss:3160.1902\n",
      "Loss:3160.0515\n",
      "Loss:3159.9151\n",
      "Loss:3159.8010\n",
      "Loss:3159.7097\n",
      "Loss:3159.6135\n",
      "Loss:3159.5442\n",
      "Loss:3159.4522\n",
      "Loss:3159.3560\n",
      "Loss:3159.3411\n",
      "Loss:3159.2995\n",
      "Loss:3159.2392\n",
      "Loss:3159.1793\n",
      "Loss:3159.1120\n",
      "Loss:3159.0739\n",
      "Loss:3159.0290\n",
      "Loss:3159.0003\n",
      "Loss:3158.9789\n",
      "Loss:3158.9190\n",
      "Loss:3158.8853\n",
      "Loss:3158.8596\n",
      "Loss:3158.8491\n",
      "Loss:3158.8390\n",
      "Loss:3158.8279\n",
      "Loss:3158.8101\n",
      "Loss:3158.7738\n",
      "Loss:3158.7581\n",
      "Loss:3158.7318\n",
      "Loss:3158.7183\n",
      "Loss:3158.6990\n",
      "Loss:3158.6736\n",
      "Loss:3158.6441\n",
      "Loss:3158.6248\n",
      "Loss:3158.6214\n",
      "Loss:3158.6158\n",
      "Loss:3158.6044\n",
      "Loss:3158.5918\n",
      "Loss:3158.5819\n",
      "Loss:3158.5704\n",
      "Loss:3158.5537\n",
      "Loss:3158.5474\n",
      "Loss:3158.5384\n",
      "Loss:3158.5323\n",
      "Loss:3158.5279\n",
      "Loss:3158.5212\n",
      "Loss:3158.5186\n",
      "Loss:3158.5169\n",
      "Loss:3158.5153\n",
      "Loss:3158.5122\n",
      "Loss:3158.5082\n",
      "Loss:3158.5004\n",
      "Loss:3158.4972\n",
      "Loss:3158.4936\n",
      "Loss:3158.4910\n",
      "Loss:3158.4880\n",
      "Loss:3158.4839\n",
      "Loss:3158.4799\n",
      "Loss:3158.4773\n",
      "Loss:3158.4746\n",
      "Loss:3158.4709\n",
      "Loss:3158.4671\n",
      "Loss:3158.4644\n",
      "Loss:3158.4631\n",
      "Loss:3158.4622\n",
      "Loss:3158.4603\n",
      "Loss:3158.4573\n",
      "Loss:3158.4565\n",
      "Loss:3158.4535\n",
      "Loss:3158.4516\n",
      "Loss:3158.4507\n",
      "Loss:3158.4498\n",
      "Loss:3158.4484\n",
      "Loss:3158.4463\n",
      "Loss:3158.4458\n",
      "Loss:3158.4449\n",
      "Loss:3158.4444\n",
      "Loss:3158.4436\n",
      "Loss:3158.4421\n",
      "Loss:3158.4416\n",
      "Loss:3158.4402\n",
      "Loss:3158.4393\n",
      "Loss:3158.4380\n",
      "Loss:3158.4376\n",
      "Loss:3158.4366\n",
      "Loss:3158.4363\n",
      "Loss:3158.4360\n",
      "Loss:3158.4358\n",
      "Loss:3158.4356\n",
      "Loss:3158.4348\n",
      "Loss:3158.4342\n",
      "Loss:3158.4338\n",
      "Loss:3158.4336\n",
      "Loss:3158.4333\n",
      "Loss:3158.4329\n",
      "Loss:3158.4326\n",
      "Loss:3158.4324\n",
      "Loss:3158.4323\n",
      "Loss:3158.4320\n",
      "Loss:3158.4317\n",
      "Loss:3158.4316\n",
      "Loss:3158.4315\n",
      "Loss:3158.4314\n",
      "Loss:3158.4314\n",
      "Loss:3158.4313\n",
      "Loss:3158.4312\n",
      "Loss:3158.4312\n",
      "Loss:3158.4311\n",
      "Loss:3158.4310\n",
      "Loss:3158.4310\n",
      "Loss:3158.4309\n",
      "Loss:3158.4309\n",
      "Loss:3158.4308\n",
      "Loss:3158.4306\n",
      "Loss:3158.4306\n",
      "Loss:3158.4306\n",
      "52313.662845134735 seconds have passed\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "T = len(tag2index)\n",
    "V = len(word2index)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def callbackF(w):\n",
    "    \"\"\"\n",
    "    This function will only be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "    w: weights, numpy array\n",
    "    \"\"\"\n",
    "    loss = get_loss_grad(w)[0]\n",
    "    print('Loss:{0:.4f}'.format(loss))\n",
    "\n",
    "def get_loss_grad(w):\n",
    "    \"\"\"\n",
    "    This function will only be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "    w: weights, numpy array\n",
    "    Returns:\n",
    "    loss: loss, float\n",
    "    grads: gradients, numpy array\n",
    "    \"\"\"\n",
    "    # to be completed by you,\n",
    "    # based on the modified loss and gradients,\n",
    "    # with L2 regularization included\n",
    "    with HiddenPrints():\n",
    "        transition_weight = w[:(T+1)*(T+1)].reshape((T+1, T+1))\n",
    "        emission_weight = w[(T+1)*(T+1):].reshape((T, V))\n",
    "        loss = LossDatasetRegularization(train_X, train_Y, tag2index, \n",
    "                                         emission_weight, transition_weight, Regularization)\n",
    "        grads_transition = GradientTransitionAllFastRegularization(train_X, train_Y, tag2index, word2index, \n",
    "                                                emission_weight, transition_weight, Regularization)\n",
    "        grads_emission = GradientEmissionAllFastRegularization(train_X, train_Y, tag2index, word2index, \n",
    "                                              emission_weight, transition_weight, Regularization)\n",
    "        grads = np.concatenate((grads_transition.reshape(-1), grads_emission.reshape(-1)))\n",
    "    return loss, grads\n",
    "\n",
    "init_w = np.zeros(((T+1)*(T+1)+T*V,))\n",
    "result = fmin_l_bfgs_b(get_loss_grad, init_w, pgtol=0.01, callback=callbackF)\n",
    "\n",
    "end = time.time()\n",
    "print('{} seconds have passed'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 2097 tokens with 236 phrases; found: 157 phrases; correct: 114.\n",
      "accuracy:  54.12%; (non-O)\n",
      "accuracy:  91.08%; precision:  72.61%; recall:  48.31%; FB1:  58.02\n",
      "              art: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "              eve: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "              geo: precision:  77.19%; recall:  51.76%; FB1:  61.97  57\n",
      "              gpe: precision: 100.00%; recall:  60.00%; FB1:  75.00  15\n",
      "              nat: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "              org: precision:  50.00%; recall:  31.43%; FB1:  38.60  22\n",
      "              per: precision:  50.00%; recall:  34.38%; FB1:  40.74  22\n",
      "              tim: precision:  80.49%; recall:  62.26%; FB1:  70.21  41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(72.61146496815286, 48.30508474576271, 58.01526717557252)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_weight, final_loss, _ = result\n",
    "transition_weight_optimal = optimal_weight[:(T+1)*(T+1)].reshape((T+1, T+1))\n",
    "emission_weight_optimal = optimal_weight[(T+1)*(T+1):].reshape((T, V))\n",
    "sequence_evaluation(test_X, test_Y, tag2index, emission_weight_optimal, transition_weight_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing predictions\n"
     ]
    }
   ],
   "source": [
    "dev_out_path_p4 = partial/'dev.p4.out'\n",
    "viterbi_output(dev_out_path, devin_ds[0], tag2index, emission_weight, transition_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with the new implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:13.0610\n",
      "Loss:8.0035\n",
      "Loss:6.4260\n",
      "Loss:5.0369\n",
      "Loss:4.6725\n",
      "Loss:4.6349\n",
      "Loss:4.6184\n",
      "Loss:4.6063\n",
      "Loss:4.5948\n",
      "Loss:4.5935\n",
      "Loss:4.5933\n",
      "Loss:4.5931\n",
      "10.841186046600342 seconds have passed\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "T = len(tag2index)\n",
    "V = len(word2index)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def callbackF(w):\n",
    "    \"\"\"\n",
    "    This function will only be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "    w: weights, numpy array\n",
    "    \"\"\"\n",
    "    loss = get_loss_grad(w)[0]\n",
    "    print('Loss:{0:.4f}'.format(loss))\n",
    "\n",
    "def get_loss_grad(w):\n",
    "    \"\"\"\n",
    "    This function will only be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "    w: weights, numpy array\n",
    "    Returns:\n",
    "    loss: loss, float\n",
    "    grads: gradients, numpy array\n",
    "    \"\"\"\n",
    "    # to be completed by you,\n",
    "    # based on the modified loss and gradients,\n",
    "    # with L2 regularization included\n",
    "    with HiddenPrints():\n",
    "        transition_weight = w[:(T+1)*(T+1)].reshape((T+1, T+1))\n",
    "        emission_weight = w[(T+1)*(T+1):].reshape((T, V))\n",
    "        loss = LossDatasetRegularization(train_X[:2], train_Y[:2], tag2index, \n",
    "                                         emission_weight, transition_weight, Regularization)\n",
    "        grads_transition = GradientTransitionAllFastRegularization(train_X[:2], train_Y[:2], tag2index, word2index, \n",
    "                                                emission_weight, transition_weight, Regularization)\n",
    "        grads_emission = GradientEmissionAllFastRegularization(train_X[:2], train_Y[:2], tag2index, word2index, \n",
    "                                              emission_weight, transition_weight, Regularization)\n",
    "        grads = np.concatenate((grads_transition.reshape(-1), grads_emission.reshape(-1)))\n",
    "    return loss, grads\n",
    "\n",
    "init_w = np.zeros(((T+1)*(T+1)+T*V,))\n",
    "result = fmin_l_bfgs_b(get_loss_grad, init_w, pgtol=0.01, callback=callbackF)\n",
    "\n",
    "end = time.time()\n",
    "print('{} seconds have passed'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTransition(epsilon, Xs, Ys, tag1, tag2, tag2index, word2index, emission_weight, transition_weight, param=0.1):\n",
    "    transition_weight_copy = copy.deepcopy(transition_weight)\n",
    "    transition_weight_copy[tag1, tag2] += epsilon\n",
    "    old_loss = LossDatasetRegularization(Xs, Ys, tag2index, emission_weight, transition_weight, param)\n",
    "    new_loss = LossDatasetRegularization(Xs, Ys, tag2index, emission_weight, transition_weight_copy, param)\n",
    "    gradient = GradientTransitionAllFastRegularization(Xs, Ys, tag2index, word2index, emission_weight, transition_weight, param)\n",
    "    print('Actual loss change: {}, Change according to gradient: {}'.format(new_loss - old_loss, gradient[tag1, tag2]*epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition: done with the 100th instances\n",
      "Transition: done with the 200th instances\n",
      "Transition: done with the 300th instances\n",
      "Transition: done with the 400th instances\n",
      "Transition: done with the 500th instances\n",
      "Transition: done with the 600th instances\n",
      "Transition: done with the 700th instances\n",
      "Actual loss change: -0.00021317379105312284, Change according to gradient: -0.0002133992138216281\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.0001\n",
    "testTransition(epsilon, train_X, train_Y, tag2index['B-per'], tag2index['I-per'], tag2index, word2index, emission_weight, transition_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testEmission(epsilon, Xs, Ys, tag, word, tag2index, word2index, emission_weight, transition_weight, param=0.1):\n",
    "    emission_weight_copy = copy.deepcopy(emission_weight)\n",
    "    emission_weight_copy[tag, word] += epsilon\n",
    "    old_loss = LossDatasetRegularization(Xs, Ys, tag2index, emission_weight, transition_weight, param)\n",
    "    new_loss = LossDatasetRegularization(Xs, Ys, tag2index, emission_weight_copy, transition_weight, param)\n",
    "    gradient = GradientEmissionAllFastRegularization(Xs, Ys, tag2index, word2index, emission_weight, transition_weight, param)\n",
    "    print('Actual loss change: {}, Change according to gradient: {}'.format(new_loss - old_loss, gradient[tag, word]*epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission: done with the 100th instances\n",
      "Emission: done with the 200th instances\n",
      "Emission: done with the 300th instances\n",
      "Emission: done with the 400th instances\n",
      "Emission: done with the 500th instances\n",
      "Emission: done with the 600th instances\n",
      "Emission: done with the 700th instances\n",
      "Actual loss change: -2.1004194422857836e-05, Change according to gradient: -2.1028369081452903e-05\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.0001\n",
    "testEmission(epsilon, train_X, train_Y, tag2index['O'], word2index['the'], tag2index, word2index, emission_weight, transition_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition: done with the 100th instances\n",
      "Transition: done with the 200th instances\n",
      "Transition: done with the 300th instances\n",
      "Transition: done with the 400th instances\n",
      "Transition: done with the 500th instances\n",
      "Transition: done with the 600th instances\n",
      "Transition: done with the 700th instances\n",
      "Actual loss change: -0.015170809550909325, Change according to gradient: -0.015171093750000001\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.0001\n",
    "testTransition(epsilon, train_X, train_Y, tag2index['B-per'], tag2index['I-per'], tag2index, word2index, np.zeros((len(tag2index), len(word2index))), np.zeros((len(tag2index)+1, len(tag2index)+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "glove_data_file = '../Embeddings/glove.6B/glove.6B.50d.txt'\n",
    "words = pd.read_table(glove_data_file, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(w):\n",
    "    return words.loc[w].to_numpy() if w in words.index else np.zeros(50,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
