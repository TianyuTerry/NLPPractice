{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial='./data/partial'\n",
    "partial_train_path = './data/partial/train'\n",
    "partial_devin_path = './data/partial/dev.in'\n",
    "partial_devout_path = './data/partial/dev.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "partial = Path('./data/partial')\n",
    "full = Path('./data/full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelled(path):\n",
    "    with open(path) as f:  \n",
    "        X, Y, x, y = list(), list(), list(), list()\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "                x, y = list(), list()\n",
    "            else:\n",
    "                word, tag = line.strip().split()\n",
    "                x.append(word)\n",
    "                y.append(tag)\n",
    "    return X, Y\n",
    "\n",
    "def unlabelled(path):\n",
    "    with open(path) as f:  \n",
    "        X, x = list(), list()\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                X.append(x)\n",
    "                x = list()\n",
    "            else:\n",
    "                word = line.strip()\n",
    "                x.append(word)\n",
    "    return X\n",
    "\n",
    "def read_data(root):\n",
    "    train, devin, devout = root/'train', root/'dev.in', root/'dev.out'     \n",
    "    return labelled(train), unlabelled(devin), labelled(devout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, devin_ds, devout_ds = read_data(partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag2index(Y):\n",
    "    tags = list(set([word for sentence in Y for word in sentence]))\n",
    "    return {tag: i for i, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2index = get_tag2index(train_ds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def emission_weight(train_ds):\n",
    "    def str_(x, y):\n",
    "        return 'emission:' + str(y) + '+' + str(x)\n",
    "    \n",
    "    vocabulary = list(set([word for sentence in train_ds[0] for word in sentence]))\n",
    "    tags = list(set([word for sentence in train_ds[1] for word in sentence]))\n",
    "    word2index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    tag2index = {tag: i for i, tag in enumerate(tags)}\n",
    "    count_table = np.zeros((len(tags), len(vocabulary)))\n",
    "    for X, Y in zip(train_ds[0], train_ds[1]):\n",
    "        for word, tag in zip(X, Y):\n",
    "            count_table[tag2index[tag], word2index[word]] += 1\n",
    "            \n",
    "    count_table/=count_table.sum(1)[:, None]\n",
    "    \n",
    "    dict_output = dict()\n",
    "    for i in range(len(tags)):\n",
    "        for j in range(len(vocabulary)):\n",
    "            if count_table[i, j] != 0:\n",
    "                dict_output[str_(vocabulary[j], tags[i])] = np.log(count_table[i, j])\n",
    "    \n",
    "    return dict_output             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_dict = emission_weight(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def emission_weight_alternative(train_ds):\n",
    "    def str_(x, y):\n",
    "        return 'emission:' + str(y) + '+' + str(x)\n",
    "    \n",
    "    dict_count = defaultdict(lambda: defaultdict(int))\n",
    "    for X, Y in zip(train_ds[0], train_ds[1]):\n",
    "        for word, tag in zip(X, Y):\n",
    "            dict_count[tag][word] += 1\n",
    "    \n",
    "    dict_output = dict()\n",
    "    for tag, words in dict_count.items():\n",
    "        tag_count = sum(words.values())\n",
    "        for word in words:\n",
    "            dict_output[str_(word, tag)] = np.log(words[word]/tag_count)\n",
    "    \n",
    "    return dict_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_dict_alternative = emission_weight_alternative(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.9318256327243257, -3.9318256327243257)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_dict['emission:I-tim+decades'], emission_dict_alternative['emission:I-tim+decades']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.333515843240958, -4.333515843240958)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_dict['emission:O+The'], emission_dict_alternative['emission:O+The']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(emission_dict.keys()) == len(emission_dict_alternative.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.817712325956905, -3.817712325956905)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_dict['emission:B-per+John'], emission_dict_alternative['emission:B-per+John']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission_weight_smooth(train_ds, k):\n",
    "    def str_(x, y):\n",
    "        return 'emission:' + str(y) + '+' + str(x)\n",
    "    \n",
    "    vocabulary = list(set([word for sentence in train_ds[0] for word in sentence]))\n",
    "    tags = list(set([word for sentence in train_ds[1] for word in sentence]))\n",
    "    word2index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    tag2index = {tag: i for i, tag in enumerate(tags)}\n",
    "    count_table = np.zeros((len(tags), len(vocabulary)))\n",
    "    for X, Y in zip(train_ds[0], train_ds[1]):\n",
    "        for word, tag in zip(X, Y):\n",
    "            count_table[tag2index[tag], word2index[word]] += 1\n",
    "    \n",
    "    removed_index = np.sum(count_table, 0) < k\n",
    "    \n",
    "    print('Number of removed words:', np.sum(removed_index))\n",
    "    print('Total number of words:', len(vocabulary))\n",
    "    \n",
    "    if (np.sum(removed_index) > 0):\n",
    "        \n",
    "        count_table = np.append(count_table, np.sum(count_table[:, removed_index], 1)[:,None], 1)\n",
    "        count_table = np.delete(count_table, np.nonzero(removed_index), 1)\n",
    "        \n",
    "        new_vocab = [vocabulary[j] for j in range(len(vocabulary)) if not removed_index[j]]+['#UNK#']\n",
    "        word2index = {w:i for i,w in enumerate(new_vocab)}\n",
    "    \n",
    "    count_table/=count_table.sum(1)[:, None]\n",
    "    \n",
    "    dict_output = dict()\n",
    "    for i in range(len(tags)):\n",
    "        for j in range(len(new_vocab)):\n",
    "            if count_table[i, j] != 0:\n",
    "                dict_output[str_(new_vocab[j], tags[i])] = np.log(count_table[i, j])\n",
    "            else:\n",
    "                dict_output[str_(new_vocab[j], tags[i])] = -np.inf\n",
    "    \n",
    "    return dict_output, word2index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed words: 2528\n",
      "Total number of words: 4068\n"
     ]
    }
   ],
   "source": [
    "emission_smth_dict, word2index = emission_weight_smooth(train_ds, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0043016091968684, -0.5253267144567906)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_smth_dict['emission:B-per+#UNK#'], emission_smth_dict['emission:I-per+#UNK#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission_weight_all(train_ds):\n",
    "    def str_(x, y):\n",
    "        return 'emission:' + str(y) + '+' + str(x)\n",
    "    \n",
    "    vocabulary = list(set([word for sentence in train_ds[0] for word in sentence]))+['#UNK#']\n",
    "    tags = list(set([word for sentence in train_ds[1] for word in sentence]))\n",
    "    word2index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    tag2index = {tag: i for i, tag in enumerate(tags)}\n",
    "    count_table = np.zeros((len(tags), len(vocabulary)))\n",
    "    for X, Y in zip(train_ds[0], train_ds[1]):\n",
    "        for word, tag in zip(X, Y):\n",
    "            count_table[tag2index[tag], word2index[word]] += 1\n",
    "    \n",
    "    count_table[:, -1] = 1\n",
    "            \n",
    "    count_table/=count_table.sum(1)[:, None]\n",
    "    \n",
    "    dict_output = dict()\n",
    "    for i in range(len(tags)):\n",
    "        for j in range(len(vocabulary)):\n",
    "            if count_table[i, j] != 0:\n",
    "                dict_output[str_(vocabulary[j], tags[i])] = np.log(count_table[i, j])\n",
    "            else:\n",
    "                dict_output[str_(vocabulary[j], tags[i])] = -np.inf\n",
    "    \n",
    "    return dict_output             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_dict_all = emission_weight_all(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.71042701737487, -6.278521424165844)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_dict_all['emission:I-per+#UNK#'], emission_dict_all['emission:B-geo+#UNK#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import copy\n",
    "\n",
    "def transition_weight(train_ds):\n",
    "    def str_(y1, y2):\n",
    "        return 'transition:' + str(y1) + '+' + str(y2)\n",
    "    \n",
    "    def pairwise(iterable):\n",
    "        a, b = itertools.tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "    \n",
    "    dict_count = defaultdict(lambda: defaultdict(int))\n",
    "    for Y in train_ds[1]:\n",
    "        Y = copy.deepcopy(Y)\n",
    "        Y.insert(0, 'START')\n",
    "        Y.append('STOP')\n",
    "        for tag1, tag2 in pairwise(Y):\n",
    "            dict_count[tag1][tag2] += 1\n",
    "    \n",
    "    dict_output = dict()\n",
    "    for tag1, tag2s in dict_count.items():\n",
    "        tag1_count = sum(tag2s.values())\n",
    "        for tag2 in tag2s:\n",
    "            dict_output[str_(tag1, tag2)] = np.log(tag2s[tag2]/tag1_count)\n",
    "    \n",
    "    return dict_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_dict = transition_weight(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_weight_alternative(train_ds):\n",
    "    def str_(y1, y2):\n",
    "        return 'transition:' + str(y1) + '+' + str(y2)\n",
    "    \n",
    "    def pairwise(iterable):\n",
    "        a, b = itertools.tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "    \n",
    "    tags = list(set([word for sentence in train_ds[1] for word in sentence]).union({'START'})) + ['STOP']\n",
    "    tag2index = {tag: i for i, tag in enumerate(tags)}\n",
    "    count_table = np.zeros((len(tags)-1, len(tags)))\n",
    "    for Y in train_ds[1]:\n",
    "        Y = copy.deepcopy(Y)\n",
    "        Y.insert(0, 'START')\n",
    "        Y.append('STOP')\n",
    "        for tag1, tag2 in pairwise(Y):\n",
    "            count_table[tag2index[tag1], tag2index[tag2]] += 1\n",
    "            \n",
    "    count_table/=count_table.sum(1)[:, None]\n",
    "    dict_output = dict()\n",
    "    for i in range(len(tags)-1):\n",
    "        for j in range(len(tags)):\n",
    "            if count_table[i, j] != 0:\n",
    "                dict_output[str_(tags[i], tags[j])] = np.log(count_table[i, j])\n",
    "    \n",
    "    return dict_output               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_dict_alternative = transition_weight_alternative(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(transition_dict.keys()) == len(transition_dict_alternative.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.5945122622174248, -1.5945122622174248)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_dict['transition:B-geo+I-geo'], transition_dict_alternative['transition:B-geo+I-geo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.744417845273085, -2.744417845273085)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_dict['transition:START+B-geo'], transition_dict_alternative['transition:START+B-geo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_weight_all(train_ds):\n",
    "    def str_(y1, y2):\n",
    "        return 'transition:' + str(y1) + '+' + str(y2)\n",
    "    \n",
    "    def pairwise(iterable):\n",
    "        a, b = itertools.tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "    \n",
    "    tags = list(set([word for sentence in train_ds[1] for word in sentence]).union({'START'})) + ['STOP']\n",
    "    tag2index = {tag: i for i, tag in enumerate(tags)}\n",
    "    count_table = np.zeros((len(tags)-1, len(tags)))\n",
    "    for Y in train_ds[1]:\n",
    "        Y = copy.deepcopy(Y)\n",
    "        Y.insert(0, 'START')\n",
    "        Y.append('STOP')\n",
    "        for tag1, tag2 in pairwise(Y):\n",
    "            count_table[tag2index[tag1], tag2index[tag2]] += 1\n",
    "            \n",
    "    count_table/=count_table.sum(1)[:, None]\n",
    "    dict_output = dict()\n",
    "    for i in range(len(tags)-1):\n",
    "        for j in range(len(tags)):\n",
    "            if count_table[i, j] != 0:\n",
    "                dict_output[str_(tags[i], tags[j])] = np.log(count_table[i, j])\n",
    "            else:\n",
    "                dict_output[str_(tags[i], tags[j])] = -np.inf\n",
    "    \n",
    "    return dict_output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_dict_all = transition_weight_all(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.744417845273085"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_dict_all['transition:START+B-geo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sequence(X, Y, emission_dict, transition_dict):\n",
    "    # First evaluate emission features\n",
    "    def str_emission(x, y):\n",
    "        return 'emission:' + str(y) + '+' + str(x)\n",
    "    def str_transition(y1, y2):\n",
    "        return 'transition:' + str(y1) + '+' + str(y2)\n",
    "    def pairwise(iterable):\n",
    "        a, b = itertools.tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "    score = 0\n",
    "    Y = copy.deepcopy(Y)\n",
    "    Y.insert(0, 'START')\n",
    "    Y.append('STOP')\n",
    "    for word, tag in zip(X, Y):\n",
    "        if str_emission(word, tag) in emission_dict:\n",
    "            score += emission_dict[str_emission(word, tag)]\n",
    "    for tag1, tag2 in pairwise(Y):\n",
    "        if str_transition(tag1, tag2) in transition_dict:\n",
    "            score += transtion_dict[str_transition(tag1, tag2)]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_emission(x, y):\n",
    "    return 'emission:' + str(y) + '+' + str(x)\n",
    "def str_transition(y1, y2):\n",
    "    return 'transition:' + str(y1) + '+' + str(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(X, tag2index, emission_dict, transition_dict):\n",
    "    \n",
    "    index2tag = {value: key for key, value in tag2index.items()}\n",
    "    score_matrix = np.zeros((len(tag2index), len(X)))\n",
    "    path_matrix = np.zeros((len(tag2index), len(X)), dtype='int')\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        if i == 0:\n",
    "            for tag2 in tag2index:\n",
    "                emission_key = str_emission(X[i], tag2)\n",
    "                default_key = str_emission('#UNK#', tag2)\n",
    "                emission_score = emission_dict[emission_key] if emission_key in emission_dict else emission_dict[default_key]\n",
    "                transition_key = str_transition('START', tag2)\n",
    "                transition_score = transition_dict[transition_key]\n",
    "                score_matrix[tag2index[tag2], i] = transition_score + emission_score\n",
    "        else:\n",
    "            for tag2 in tag2index:\n",
    "                competitors = np.zeros((len(tag2index)))\n",
    "                for tag1 in tag2index:\n",
    "                    emission_key = str_emission(X[i], tag2)\n",
    "                    default_key = str_emission('#UNK#', tag2)\n",
    "                    emission_score = emission_dict[emission_key] if emission_key in emission_dict else emission_dict[default_key]\n",
    "                    transition_key = str_transition(tag1, tag2)\n",
    "                    transition_score = transition_dict[transition_key]\n",
    "                    competitors[tag2index[tag1]] = score_matrix[tag2index[tag1], i-1] + transition_score + emission_score\n",
    "                score_matrix[tag2index[tag2], i] = np.max(competitors)\n",
    "                path_matrix[tag2index[tag2], i] = np.argmax(competitors)\n",
    "    competitors = np.zeros((len(tag2index)))\n",
    "    for tag1 in tag2index:\n",
    "        transition_key = str_transition(tag1, 'STOP')\n",
    "        transition_score = transition_dict[transition_key]\n",
    "        competitors[tag2index[tag1]] = score_matrix[tag2index[tag1], -1] + transition_score\n",
    "    \n",
    "#     for i in range(len(X)):\n",
    "#         print('**************************')\n",
    "#         print(path_matrix[:, i])\n",
    "#         print(score_matrix[:, i])\n",
    "    \n",
    "    last_idx = np.argmax(competitors)\n",
    "    path = [last_idx]\n",
    "    for m in range(len(X)-1, 0, -1):\n",
    "        path.insert(0, path_matrix[path[0], m])\n",
    "    output_tags = [index2tag[idx] for idx in path]\n",
    "    return output_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for one case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['O',\n",
       "  'O',\n",
       "  'B-org',\n",
       "  'I-org',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-org',\n",
       "  'I-org',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'B-org',\n",
       "  'I-org',\n",
       "  'I-org',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-org',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi(devout_ds[0][0], tag2index, emission_smth_dict, transition_dict_all), devout_ds[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_output(dev_out_path, X, tag2index, emission_dict, transition_dict):\n",
    "    \n",
    "    tags = [viterbi(sentence, tag2index, emission_dict, transition_dict) for sentence in X]\n",
    "    \n",
    "    output_string = ''\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[i])):\n",
    "            output_string += X[i][j] + ' ' + tags[i][j] + '\\n'\n",
    "        output_string += '\\n'\n",
    "    \n",
    "    with open(dev_out_path, 'w') as f:\n",
    "        f.write(output_string)\n",
    "    \n",
    "    print('Done with writing predictions')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing predictions\n"
     ]
    }
   ],
   "source": [
    "dev_out_path = partial/'dev.p2.out'\n",
    "viterbi_output(dev_out_path, devin_ds[0], tag2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conlleval_ import evaluate\n",
    "\n",
    "def sequence_evaluation(X, Y, tag2index, emission_dict, transition_dict):\n",
    "    tags_ = [viterbi(sentence, tag2index, emission_dict, transition_dict) for sentence in X]\n",
    "    tags = [tag for tags in tags_ for tag in tags]\n",
    "    Y  = [tag for tags in Y for tag in tags]\n",
    "    assert len(Y) == len(tags)\n",
    "    return evaluate(Y, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 2097 tokens with 236 phrases; found: 165 phrases; correct: 114.\n",
      "accuracy:  53.82%; (non-O)\n",
      "accuracy:  89.94%; precision:  69.09%; recall:  48.31%; FB1:  56.86\n",
      "              art: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "              eve: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "              geo: precision:  76.36%; recall:  49.41%; FB1:  60.00  55\n",
      "              gpe: precision:  88.89%; recall:  64.00%; FB1:  74.42  18\n",
      "              nat: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "              org: precision:  41.67%; recall:  28.57%; FB1:  33.90  24\n",
      "              per: precision:  50.00%; recall:  37.50%; FB1:  42.86  24\n",
      "              tim: precision:  77.27%; recall:  64.15%; FB1:  70.10  44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(69.0909090909091, 48.30508474576271, 56.85785536159601)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_evaluation(devin_ds, devout_ds[1], tag2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 2097 tokens with 236 phrases; found: 380 phrases; correct: 117.\n",
      "accuracy:  53.82%; (non-O)\n",
      "accuracy:  77.54%; precision:  30.79%; recall:  49.58%; FB1:  37.99\n",
      "              art: precision:   0.00%; recall:   0.00%; FB1:   0.00  18\n",
      "              eve: precision:   0.00%; recall:   0.00%; FB1:   0.00  21\n",
      "              geo: precision:  65.79%; recall:  58.82%; FB1:  62.11  76\n",
      "              gpe: precision:  40.62%; recall:  52.00%; FB1:  45.61  32\n",
      "              nat: precision:   0.00%; recall:   0.00%; FB1:   0.00  131\n",
      "              org: precision:  33.33%; recall:  28.57%; FB1:  30.77  30\n",
      "              per: precision:  60.00%; recall:  37.50%; FB1:  46.15  20\n",
      "              tim: precision:  61.54%; recall:  60.38%; FB1:  60.95  52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30.789473684210527, 49.57627118644068, 37.98701298701299)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_evaluation(devin_ds, devout_ds[1], tag2index, emission_dict_all, transition_dict_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It turns out that smoothing method introduced in ML course perform better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, tag2index, emission_dict, transition_dict):\n",
    "    \"\"\"\n",
    "    The returned value should be the forward matrix for sentence X and the final alpha\n",
    "    The implementation so far explicitly pass emission and transition dictionary as arguments\n",
    "    This can be replaced by FeatureSum function in the implementation for part 5\n",
    "    \"\"\"\n",
    "    forward_matrix = np.zeros((len(tag2index), len(X)))\n",
    "    emission_dict_exp = {k: np.exp(v) for (k, v) in emission_dict.items()}\n",
    "    transition_dict_exp = {k: np.exp(v) for (k, v) in transition_dict.items()}\n",
    "    for i in range(len(X)):\n",
    "        if i == 0:\n",
    "            for tag2 in tag2index:\n",
    "                emission_key = str_emission(X[i], tag2)\n",
    "                default_key = str_emission('#UNK#', tag2)\n",
    "                emission_score = emission_dict_exp[emission_key] if emission_key in emission_dict_exp else emission_dict_exp[default_key]\n",
    "                transition_key = str_transition('START', tag2)\n",
    "                transition_score = transition_dict_exp[transition_key]\n",
    "                forward_matrix[tag2index[tag2], i] = transition_score*emission_score\n",
    "        else:\n",
    "            for tag2 in tag2index:\n",
    "                SumPotential = 0\n",
    "                for tag1 in tag2index:\n",
    "                    emission_key = str_emission(X[i], tag2)\n",
    "                    default_key = str_emission('#UNK#', tag2)\n",
    "                    emission_score = emission_dict_exp[emission_key] if emission_key in emission_dict_exp else emission_dict_exp[default_key]\n",
    "                    transition_key = str_transition(tag1, tag2)\n",
    "                    transition_score = transition_dict_exp[transition_key]\n",
    "                    SumPotential += forward_matrix[tag2index[tag1], i-1]*transition_score*emission_score\n",
    "                forward_matrix[tag2index[tag2], i] = SumPotential\n",
    "    \n",
    "    SumPotential = 0\n",
    "    for tag1 in tag2index:\n",
    "        transition_key = str_transition(tag1, 'STOP')\n",
    "        transition_score = transition_dict_exp[transition_key]\n",
    "        SumPotential += forward_matrix[tag2index[tag1], -1]*transition_score\n",
    "    \n",
    "    return forward_matrix, SumPotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(X, Y, tag2index, emission_dict, transition_dict):\n",
    "    \n",
    "    pair_score = 0\n",
    "    for i in range(len(X)):\n",
    "        if i == 0:\n",
    "            emission_key = str_emission(X[i], Y[i])\n",
    "            default_key = str_emission('#UNK#', Y[i])\n",
    "            emission_score = emission_dict[emission_key] if emission_key in emission_dict else emission_dict[default_key]\n",
    "            transition_key = str_transition('START', Y[i])\n",
    "            transition_score = transition_dict[transition_key]\n",
    "            pair_score += (transition_score+emission_score)\n",
    "        else:\n",
    "            emission_key = str_emission(X[i], Y[i])\n",
    "            default_key = str_emission('#UNK#', Y[i])\n",
    "            emission_score = emission_dict[emission_key] if emission_key in emission_dict else emission_dict[default_key]\n",
    "            transition_key = str_transition(Y[i-1], Y[i])\n",
    "            transition_score = transition_dict[transition_key]\n",
    "            pair_score += (transition_score+emission_score)\n",
    "    transition_key = str_transition(Y[-1], 'STOP')\n",
    "    transition_score = transition_dict[transition_key]\n",
    "    pair_score += transition_score\n",
    "    \n",
    "    _, SumPotential = forward(X, tag2index, emission_dict, transition_dict)\n",
    "    \n",
    "    return -(pair_score-np.log(SumPotential))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossDataset(Xs, Ys, tag2index, emission_dict, transition_dict):\n",
    "    return np.sum([Loss(X, Y, tag2index, emission_dict, transition_dict) for X, Y in zip(Xs, Ys)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156.7293881993182"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LossDataset(train_ds[0], train_ds[1], tag2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_alternative(X, tag2index, emission_dict, transition_dict):\n",
    "    \"\"\"\n",
    "    The returned value should be the forward matrix for sentence X and the final beta\n",
    "    final beta should be the same as final alpha\n",
    "    \"\"\"\n",
    "    backward_matrix = np.zeros((len(tag2index), len(X)))\n",
    "    emission_dict_exp = {k: np.exp(v) for (k, v) in emission_dict.items()}\n",
    "    transition_dict_exp = {k: np.exp(v) for (k, v) in transition_dict.items()}\n",
    "    for i in range(len(X)-1, -1, -1):\n",
    "        if i == len(X)-1:\n",
    "            for tag1 in tag2index:\n",
    "                emission_key = str_emission(X[i], tag1)\n",
    "                default_key = str_emission('#UNK#', tag1)\n",
    "                emission_score = emission_dict_exp[emission_key] if emission_key in emission_dict_exp else emission_dict_exp[default_key]\n",
    "                transition_key = str_transition(tag1, 'STOP')\n",
    "                transition_score = transition_dict_exp[transition_key]\n",
    "                backward_matrix[tag2index[tag1], i] = transition_score*emission_score\n",
    "        else:\n",
    "            for tag1 in tag2index:\n",
    "                SumPotential = 0\n",
    "                for tag2 in tag2index:\n",
    "                    emission_key = str_emission(X[i], tag1)\n",
    "                    default_key = str_emission('#UNK#', tag1)\n",
    "                    emission_score = emission_dict_exp[emission_key] if emission_key in emission_dict_exp else emission_dict_exp[default_key]\n",
    "                    transition_key = str_transition(tag1, tag2)\n",
    "                    transition_score = transition_dict_exp[transition_key]\n",
    "                    SumPotential += backward_matrix[tag2index[tag2], i+1]*transition_score*emission_score\n",
    "                backward_matrix[tag2index[tag1], i] = SumPotential\n",
    "    \n",
    "    SumPotential = 0\n",
    "    for tag2 in tag2index:\n",
    "        transition_key = str_transition('START', tag2)\n",
    "        transition_score = transition_dict_exp[transition_key]\n",
    "        SumPotential += backward_matrix[tag2index[tag2], 0]*transition_score\n",
    "    \n",
    "    return backward_matrix, SumPotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, tag2index, emission_dict, transition_dict):\n",
    "    \"\"\"\n",
    "    The returned value should be the forward matrix for sentence X and the final beta\n",
    "    final beta should be the same as final alpha\n",
    "    This is considered correct according to the definition of backward algorithm\n",
    "    \"\"\"\n",
    "    backward_matrix = np.zeros((len(tag2index), len(X)))\n",
    "    emission_dict_exp = {k: np.exp(v) for (k, v) in emission_dict.items()}\n",
    "    transition_dict_exp = {k: np.exp(v) for (k, v) in transition_dict.items()}\n",
    "    \n",
    "    for tag1 in tag2index:\n",
    "        transition_key = str_transition(tag1, 'STOP')\n",
    "        transition_score = transition_dict_exp[transition_key]\n",
    "        backward_matrix[tag2index[tag1], -1] = transition_score\n",
    "    \n",
    "    for i in range(len(X)-2, -1, -1):\n",
    "        for tag1 in tag2index:\n",
    "            SumPotential = 0\n",
    "            for tag2 in tag2index:\n",
    "                emission_key = str_emission(X[i+1], tag2)\n",
    "                default_key = str_emission('#UNK#', tag2)\n",
    "                emission_score = emission_dict_exp[emission_key] if emission_key in emission_dict_exp else emission_dict_exp[default_key]\n",
    "                transition_key = str_transition(tag1, tag2)\n",
    "                transition_score = transition_dict_exp[transition_key]\n",
    "                SumPotential += backward_matrix[tag2index[tag2], i+1]*transition_score*emission_score\n",
    "            backward_matrix[tag2index[tag1], i] = SumPotential\n",
    "    \n",
    "    SumPotential = 0\n",
    "    for tag2 in tag2index:\n",
    "        emission_key = str_emission(X[0], tag2)\n",
    "        default_key = str_emission('#UNK#', tag2)\n",
    "        emission_score = emission_dict_exp[emission_key] if emission_key in emission_dict_exp else emission_dict_exp[default_key]\n",
    "        transition_key = str_transition('START', tag2)\n",
    "        transition_score = transition_dict_exp[transition_key]\n",
    "        SumPotential += backward_matrix[tag2index[tag2], 0]*transition_score*emission_score\n",
    "    \n",
    "    return backward_matrix, SumPotential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the implemented forward-backward algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1244349940099905e-50, 1.12443499400999e-50)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, forward_sum = forward(train_ds[0][0], tag2index, emission_smth_dict, transition_dict_all)\n",
    "_, backward_sum = backward_alternative(train_ds[0][0], tag2index, emission_smth_dict, transition_dict_all)\n",
    "forward_sum, backward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, backward_sum_alt = backward_alternative(train_ds[0][0], tag2index, emission_smth_dict, transition_dict_all)\n",
    "assert backward_sum_alt == backward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_count_emission_alternative(X, tag, word, tag2index, word2index, emission_dict, transition_dict):\n",
    "    forward_matrix, NormalizationTerm = forward(X, tag2index, emission_dict, transition_dict)\n",
    "    backward_matrix, _ = backward(X, tag2index, emission_dict, transition_dict)\n",
    "    SumPotential = 0\n",
    "    X = [x if x in word2index else '#UNK#' for x in X]\n",
    "    if word not in word2index:\n",
    "        word = '#UNK#'\n",
    "    for i in range(len(X)-1):\n",
    "        if X[i] == word:\n",
    "            print('index at which the word appears:', i)\n",
    "            for tag2 in tag2index:\n",
    "                transition_score = np.exp(transition_dict[str_transition(tag, tag2)])\n",
    "                emission_score = np.exp(emission_dict[str_emission(X[i+1], tag2)])\n",
    "                SumPotential += forward_matrix[tag2index[tag], i]*backward_matrix[tag2index[tag2], i+1]*emission_score*transition_score\n",
    "    transition_score = np.exp(transition_dict[str_transition(tag, 'STOP')])\n",
    "    SumPotential += forward_matrix[tag2index[tag], -1]*transition_score\n",
    "    return SumPotential/NormalizationTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_count_emission(X, tag, word, tag2index, word2index, emission_dict, transition_dict):\n",
    "    \"\"\"\n",
    "    This is considered correct according to the definition of f(y_i-1, y_i, x_i)\n",
    "    \"\"\"\n",
    "    forward_matrix, NormalizationTerm = forward(X, tag2index, emission_dict, transition_dict)\n",
    "    backward_matrix, _ = backward(X, tag2index, emission_dict, transition_dict)\n",
    "    SumPotential = 0\n",
    "    X = [x if x in word2index else '#UNK#' for x in X]\n",
    "    if word not in word2index:\n",
    "        word = '#UNK#'\n",
    "    \n",
    "    emission_score = np.exp(emission_dict[str_emission(word, tag)])\n",
    "    if X[0] == word:\n",
    "        transition_score = np.exp(transition_dict[str_transition('START', tag)])\n",
    "        SumPotential += backward_matrix[tag2index[tag], 0]*emission_score*transition_score\n",
    "    for i in range(1, len(X)):\n",
    "        if X[i] == word:\n",
    "            print('index at which the word appears:', i)\n",
    "            for tag1 in tag2index:\n",
    "                transition_score = np.exp(transition_dict[str_transition(tag1, tag)])\n",
    "                SumPotential += forward_matrix[tag2index[tag1], i-1]*backward_matrix[tag2index[tag], i]*emission_score*transition_score\n",
    "    return SumPotential/NormalizationTerm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test means nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index at which the word appears: 1\n",
      "index at which the word appears: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.909108875859154"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_count_emission(train_ds[0][1], 'I-per', 'Omi', tag2index, word2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index at which the word appears: 1\n",
      "index at which the word appears: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9091088758591538"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_count_emission_alternative(train_ds[0][1], 'I-per', 'Omi', tag2index, word2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index at which the word appears: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47734588521399984"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_count_emission(train_ds[0][2], 'B-geo', 'DRC', tag2index, word2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index at which the word appears: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47734588521399984"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_count_emission_alternative(train_ds[0][2], 'B-geo', 'DRC', tag2index, word2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index at which the word appears: 1\n",
      "index at which the word appears: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9397328051797769"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_count_emission_alternative(train_ds[0][1], 'O', '#UNK#', tag2index, word2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index at which the word appears: 1\n",
      "index at which the word appears: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9397328051797771"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_count_emission(train_ds[0][1], 'O', '#UNK#', tag2index, word2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_count_emission(X, Y, tag, word, tag2index, word2index):\n",
    "    X = [x if x in word2index else '#UNK#' for x in X]\n",
    "    if word not in word2index:\n",
    "        word = '#UNK#'\n",
    "    count = 0\n",
    "    for x, y in zip(X, Y):\n",
    "        if x == word and y == tag:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_count_emission(train_ds[0][1], train_ds[1][1], 'O', '#UNK#', tag2index, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_count_transition(X, tag1, tag2, tag2index, word2index, emission_dict, transition_dict):\n",
    "    forward_matrix, NormalizationTerm = forward(X, tag2index, emission_dict, transition_dict)\n",
    "    backward_matrix, _ = backward(X, tag2index, emission_dict, transition_dict)\n",
    "    X = [x if x in word2index else '#UNK#' for x in X]\n",
    "    SumPotential = 0\n",
    "    transition_score = np.exp(transition_dict[str_transition(tag1, tag2)])\n",
    "    if tag1 == 'START':\n",
    "        emission_score = np.exp(emission_dict[str_emission(X[0], tag2)])\n",
    "        SumPotential += transition_score*emission_score*backward_matrix[tag2index[tag2], 0]\n",
    "    elif tag2 == 'STOP':\n",
    "        SumPotential += forward_matrix[tag2index[tag1], -1]*transition_score\n",
    "    else:\n",
    "        for i in range(len(X)-1):\n",
    "            emission_score = np.exp(emission_dict[str_emission(X[i+1], tag2)])\n",
    "            SumPotential += forward_matrix[tag2index[tag1], i]*transition_score*emission_score*backward_matrix[tag2index[tag2], i+1]\n",
    "    return SumPotential/NormalizationTerm                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_count_transition(X, Y, tag1, tag2, tag2index, word2index):\n",
    "    count = 0\n",
    "    def pairwise(iterable):\n",
    "        a, b = itertools.tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "    Y = copy.deepcopy(Y)\n",
    "    Y.append('STOP')\n",
    "    Y.insert(0, 'START')\n",
    "    for tag1_, tag2_ in pairwise(Y):\n",
    "        if tag1 == tag1_ and tag2 == tag2_:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test means nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.783140987648911"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_count_transition(train_ds[0][1], 'O', 'O', tag2index, word2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_count_transition(train_ds[0][1], 'START', 'O', tag2index, word2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_count_transition(train_ds[0][1], 'START', 'B-per', tag2index, word2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 'B-per')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds[0][1]), train_ds[1][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_count_transition(train_ds[0][1], train_ds[1][1], 'O', 'O', tag2index, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test according to instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0068095859707824275, -0.006801962072131573)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Increase 'O'-'O' by epsilon\n",
    "epsilon = 0.0001\n",
    "transition_dict_all_copy = copy.deepcopy(transition_dict_all)\n",
    "transition_dict_all_copy[str_transition('O', 'O')] += epsilon\n",
    "old_loss = LossDataset(train_ds[0], train_ds[1], tag2index, emission_smth_dict, transition_dict_all)\n",
    "new_loss = LossDataset(train_ds[0], train_ds[1], tag2index, emission_smth_dict, transition_dict_all_copy)\n",
    "expected_count = np.sum([expected_count_transition(X, 'O', 'O', tag2index, word2index, emission_smth_dict, transition_dict_all) for X in train_ds[0]])\n",
    "actual_count = np.sum([actual_count_transition(X, Y, 'O', 'O', tag2index, word2index) for (X, Y) in zip(train_ds[0], train_ds[1])])\n",
    "gradient = expected_count - actual_count\n",
    "gradient*epsilon, new_loss-old_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0003452907908894076, -0.0003453194032164845)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 0.0001\n",
    "X_test, Y_test = train_ds[0][0], train_ds[1][0]\n",
    "transition_dict_all_copy = copy.deepcopy(transition_dict_all)\n",
    "transition_dict_all_copy[str_transition('O', 'O')] += epsilon\n",
    "old_loss = Loss(X_test, Y_test, tag2index, emission_smth_dict, transition_dict_all)\n",
    "new_loss = Loss(X_test, Y_test, tag2index, emission_smth_dict, transition_dict_all_copy)\n",
    "expected_count = expected_count_transition(X_test, 'O', 'O', tag2index, word2index, emission_smth_dict, transition_dict_all)\n",
    "actual_count = actual_count_transition(X_test, Y_test, 'O', 'O', tag2index, word2index)\n",
    "gradient = expected_count - actual_count\n",
    "new_loss - old_loss, gradient*epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index at which the word appears: 12\n",
      "index at which the word appears: 13\n",
      "index at which the word appears: 15\n",
      "index at which the word appears: 16\n",
      "index at which the word appears: 20\n",
      "index at which the word appears: 21\n",
      "index at which the word appears: 22\n",
      "Actual loss change: -0.00021219469253708212, Change according to gradient: -0.00021220724764608195\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.0001\n",
    "X_test, Y_test = train_ds[0][0], train_ds[1][0]\n",
    "emission_smth_dict_copy = copy.deepcopy(emission_smth_dict)\n",
    "emission_smth_dict_copy[str_emission('#UNK#', 'O')] += epsilon\n",
    "old_loss = Loss(X_test, Y_test, tag2index, emission_smth_dict, transition_dict_all)\n",
    "new_loss = Loss(X_test, Y_test, tag2index, emission_smth_dict_copy, transition_dict_all)\n",
    "expected_count = expected_count_emission(X_test, 'O', '#UNK#', tag2index, word2index, emission_smth_dict, transition_dict_all)\n",
    "actual_count = actual_count_emission(X_test, Y_test, 'O', '#UNK#', tag2index, word2index)\n",
    "gradient = expected_count - actual_count\n",
    "print('Actual loss change: {}, Change according to gradient: {}'.format(new_loss - old_loss, gradient*epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index at which the word appears: 1\n",
      "index at which the word appears: 9\n",
      "Actual loss change: -6.02587614650929e-06, Change according to gradient: -6.026719482022292e-06\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.0001\n",
    "X_test, Y_test = train_ds[0][1], train_ds[1][1]\n",
    "emission_smth_dict_copy = copy.deepcopy(emission_smth_dict)\n",
    "emission_smth_dict_copy[str_emission('#UNK#', 'O')] += epsilon\n",
    "old_loss = Loss(X_test, Y_test, tag2index, emission_smth_dict, transition_dict_all)\n",
    "new_loss = Loss(X_test, Y_test, tag2index, emission_smth_dict_copy, transition_dict_all)\n",
    "expected_count = expected_count_emission(X_test, 'O', '#UNK#', tag2index, word2index, emission_smth_dict, transition_dict_all)\n",
    "actual_count = actual_count_emission(X_test, Y_test, 'O', '#UNK#', tag2index, word2index)\n",
    "gradient = expected_count - actual_count\n",
    "print('Actual loss change: {}, Change according to gradient: {}'.format(new_loss - old_loss, gradient*epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The only bug now is in expected_count_emission_alternative, error in some edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientEmission(Xs, Ys, tag, word, tag2index, word2index, emission_dict, transition_dict):\n",
    "    with HiddenPrints():\n",
    "        expected_count = np.sum([expected_count_emission(X, tag, word, tag2index, word2index, emission_smth_dict, transition_dict_all) for X in Xs])\n",
    "        actual_count = np.sum([actual_count_emission(X, Y, tag, word, tag2index, word2index) for X, Y in zip(Xs, Ys)])\n",
    "    return expected_count - actual_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientTransition(Xs, Ys, tag1, tag2, tag2index, word2index, emission_dict, transition_dict):\n",
    "    with HiddenPrints():\n",
    "        expected_count = np.sum([expected_count_transition(X, tag1, tag2, tag2index, word2index, emission_smth_dict, transition_dict_all) for X in Xs])\n",
    "        actual_count = np.sum([actual_count_transition(X, Y, tag1, tag2, tag2index, word2index) for X, Y in zip(Xs, Ys)])\n",
    "    return expected_count - actual_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.86508333377901"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradientEmission(train_ds[0], train_ds[1], 'O', '#UNK', tag2index, word2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-68.09585970782427"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradientTransition(train_ds[0], train_ds[1], 'O', 'O', tag2index, word2index, emission_smth_dict, transition_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientTransitionAll(Xs, Ys, tag2index, word2index, emission_dict, transition_dict):\n",
    "    i = 1\n",
    "    gradient = dict()\n",
    "    tag2index_ = copy.deepcopy(tag2index)\n",
    "    tag2index_['START'] = len(tag2index_)\n",
    "    tag2index_['STOP'] = len(tag2index_)\n",
    "    for tag1, tag2 in itertools.product(tag2index_, tag2index_):\n",
    "        if tag1 != 'STOP':\n",
    "            gradient[str_transition(tag1, tag2)] = GradientTransition(Xs, Ys, tag1, tag2, tag2index, word2index, emission_dict, transition_dict)\n",
    "            print('done with the {}th gradient'.format(i))\n",
    "            i += 1\n",
    "            if i == 4:\n",
    "                return gradient\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientEmissionAll(Xs, Ys, tag2index, word2index, emission_dict, transition_dict):\n",
    "    gradient = dict()\n",
    "    for word, tag in itertools.product(word2index, tag2index):\n",
    "        gradient[str_emission(word, tag)] = GradientEmission(Xs, Ys, tag, word, tag2index, word2index, emission_dict, transition_dict)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with the 1th gradient\n",
      "done with the 2th gradient\n",
      "done with the 3th gradient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "154.70667815208435"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "gradient_trans_dict = GradientTransitionAll(train_ds[0], train_ds[1], tag2index, word2index, emission_smth_dict, transition_dict_all)\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transition:I-gpe+I-gpe': 0.0,\n",
       " 'transition:I-gpe+I-eve': 0.0,\n",
       " 'transition:I-gpe+I-per': 0.0}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_trans_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([actual_count_transition(X, Y, 'I-gpe', 'I-gpe', tag2index, word2index) for X, Y in zip(train_ds[0], train_ds[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossDatasetRegularization(Xs, Ys, tag2index, emission_dict, transition_dict, param):\n",
    "    return LossDataset(Xs, Ys, tag2index, emission_dict, transition_dict) +\\\n",
    "        param*(np.sum([w**2 for w in emission_dict.values() if w != -np.inf]) +\\\n",
    "               np.sum([w**2 for w in transition_dict.values() if w != -np.inf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12066.057801496318"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1\n",
    "LossDatasetRegularization(train_ds[0], train_ds[1], tag2index, emission_smth_dict, transition_dict_all, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature2index(tag2index, word2index):\n",
    "    emission = [str_emission(word, tag) for word, tag in itertools.product(word2index, tag2index)]\n",
    "    tag2index = copy.deepcopy(tag2index)\n",
    "    tag2index['START'] = len(tag2index)\n",
    "    tag2index['STOP'] = len(tag2index)\n",
    "    transition = [str_transition() for tag1, tag2 in itertools.product(tag2index, tag2index) if tag1 != 'STOP']\n",
    "    return {feature: i for i, feature in enumerate(emission+transition)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructure Everything for Part 4 in CRFNumpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelled_full(path):\n",
    "    with open(path) as f:  \n",
    "        X, Y, Z, x, y, z = list(), list(), list(), list(), list(), list()\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "                Z.append(z)\n",
    "                x, y, z = list(), list(), list()\n",
    "            else:\n",
    "                word, pos_tag, tag = line.strip().split()\n",
    "                x.append(word)\n",
    "                y.append(tag)\n",
    "                z.append(pos_tag)\n",
    "    return X, Z, Y\n",
    "\n",
    "def unlabelled_full(path):\n",
    "    with open(path) as f:  \n",
    "        X, Z, x, z = list(), list(), list(), list()\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                X.append(x)\n",
    "                Z.append(z)\n",
    "                x, z = list(), list()\n",
    "            else:\n",
    "                word, pos_tag = line.strip().split()\n",
    "                x.append(word)\n",
    "                z.append(pos_tag)\n",
    "    return X, Z\n",
    "\n",
    "def read_data_full(root):\n",
    "    train, devin, devout = root/'train', root/'dev.in', root/'dev.out'     \n",
    "    return labelled_full(train), unlabelled_full(devin), labelled_full(devout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, devin_ds, devout_ds = read_data_full(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission_weight_POS(train_ds, tag2index):\n",
    "    \n",
    "    T = len(tag2index)\n",
    "    postags = list(set([postag for postags in train_ds[1] for postag in postags]))\n",
    "    postag2index = {postag: i for i, postag in enumerate(postags)}\n",
    "    T_ = len(postag2index)\n",
    "    count_table = np.zeros((T, T_))\n",
    "    for Z, Y in zip(train_ds[1], train_ds[2]):\n",
    "        for postag, tag in zip(Z, Y):\n",
    "            count_table[tag2index[tag], postag2index[postag]] += 1\n",
    "            \n",
    "    count_table/=count_table.sum(1)[:, None]\n",
    "    \n",
    "    transition_weight_pos = np.ma.log(count_table).filled(-np.inf)\n",
    "    \n",
    "    return transition_weight_pos, postag2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_weight_pos, postag2index = emission_weight_POS(train_ds, tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, (16, 40))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(postag2index), transition_weight_pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings for LSTM-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'world!'], ['good', 'day']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 4, 768)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "texts = ['hello world!', 'good day']\n",
    "tokenized_texts = [s.split() for s in texts]\n",
    "print(tokenized_texts)\n",
    "vectors = bc.encode(tokenized_texts, is_tokenized=True)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Context.__del__ at 0x1065a2840>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wutianyu/anaconda3/lib/python3.7/site-packages/zmq/sugar/context.py\", line 50, in __del__\n",
      "    self.term()\n",
      "  File \"zmq/backend/cython/context.pyx\", line 91, in zmq.backend.cython.context.Context.term\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt\n",
      "/Users/wutianyu/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = '../Embeddings/glove.6B/glove.6B.50d.txt'\n",
    "word2vec_output_file = '../Embeddings/glove.6B/glove.6B.50d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = '../Embeddings/glove.6B/glove.6B.50d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "# calculate (king - man) + woman\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "model['easy'].shape, result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
