{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "partial = Path('./data/partial')\n",
    "full = Path('./data/full')\n",
    "\n",
    "def labelled(path):\n",
    "    with open(path) as f:  \n",
    "        X, Y, x, y = list(), list(), list(), list()\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "                x, y = list(), list()\n",
    "            else:\n",
    "                word, tag = line.strip().split()\n",
    "                x.append(word)\n",
    "                y.append(tag)\n",
    "    return X, Y\n",
    "\n",
    "def unlabelled(path):\n",
    "    with open(path) as f:  \n",
    "        X, x = list(), list()\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                X.append(x)\n",
    "                x = list()\n",
    "            else:\n",
    "                word = line.strip()\n",
    "                x.append(word)\n",
    "    return X\n",
    "\n",
    "def read_data(root):\n",
    "    train, devin, devout = root/'train', root/'dev.in', root/'dev.out'     \n",
    "    return labelled(train), unlabelled(devin), labelled(devout)\n",
    "\n",
    "train_ds, devin_ds, devout_ds = read_data(partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, word2index):\n",
    "    return [word2index[word] if word in word2index else -1 for word in sentence]\n",
    "def tag2idx(tags, tag2index):\n",
    "    return [tag2index[tag] for tag in tags]\n",
    "\n",
    "def idx_xy(X, Y, word2index=None, tag2index=None):\n",
    "    if not word2index:\n",
    "        vocabulary = list(set([word for sentence in X for word in sentence]))\n",
    "        word2index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    if not tag2index:\n",
    "        tags = list(set([tag for tags in Y for tag in tags]))\n",
    "        tags = ['START'] + tags + ['STOP']\n",
    "        tag2index = {tag: i for i, tag in enumerate(tags)}\n",
    "    \n",
    "    index2tag = {v:k for (k, v) in tag2index.items()}\n",
    "    \n",
    "    X = [tokenize(sentence, word2index) for sentence in X]\n",
    "    Y = [tag2idx(tags, tag2index) for tags in Y]\n",
    "    \n",
    "    return X, Y, word2index, tag2index, index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, word2index, tag2index, index2tag = idx_xy(train_ds[0], train_ds[1])\n",
    "test_X, test_Y, _, _, _ = idx_xy(devout_ds[0], devout_ds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_emission(x, y):\n",
    "    return 'emission:' + str(y) + '+' + str(x)\n",
    "def str_transition(y1, y2):\n",
    "    return 'transition:' + str(y1) + '+' + str(y2)\n",
    "\n",
    "def feature2idx(words, tags):\n",
    "    features = list()\n",
    "    T = len(tags)\n",
    "    V = len(words)\n",
    "    for y1, y2 in itertools.product(range(T), range(T)):\n",
    "        features.append(str_transition(y1, y2))\n",
    "    for x, y in itertools.product(range(V), range(T)):\n",
    "        features.append(str_emission(x, y))\n",
    "    return {f:i for i, f in enumerate(features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature2index = feature2idx(word2index, tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_activation(x, tag2index, feature2index):\n",
    "    N, T, F = len(x), len(tag2index), len(feature2index)\n",
    "    feature_activation_map = np.zeros((N+1, T, T, F), dtype=int)\n",
    "    for i, y1, y2 in itertools.product(range(N+1), range(T), range(T)):\n",
    "        feature_activation_map[i, y1, y2][feature2index[str_transition(y1, y2)]] = 1\n",
    "        if i < N and x[i] != -1:\n",
    "            feature_activation_map[i, y1, y2][feature2index[str_emission(x[i], y2)]] = 1\n",
    "            \n",
    "    return feature_activation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(log_phi):\n",
    "    \"\"\"\n",
    "    forward_matrix of shape (T, N+2), forward_matrix[-1, -1] is log_Z\n",
    "    \"\"\"\n",
    "    N, T = log_phi.shape[0]-1, log_phi.shape[1]\n",
    "    forward_matrix = np.ones((T, N+2))*np.NINF\n",
    "    forward_matrix[0][0] = 0\n",
    "    for i in range(1, N+2):\n",
    "        forward_matrix[:, i] = logsumexp(np.expand_dims(forward_matrix[:, i-1], axis=1)+log_phi[i-1], axis=0)\n",
    "        if i < N+1:\n",
    "            forward_matrix[[0, -1], i] = np.NINF\n",
    "    return forward_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_new(log_phi):\n",
    "#     \"\"\"\n",
    "#     forward_matrix of shape (T, N+2), forward_matrix[-1, -1] is log_Z\n",
    "#     \"\"\"\n",
    "#     N, T = log_phi.shape[0]-1, log_phi.shape[1]\n",
    "#     forward_matrix = np.ones((T, N+2))*np.NINF\n",
    "#     forward_matrix[0][0] = 0\n",
    "#     forward_matrix[1:-1, 1] = logsumexp(np.expand_dims(forward_matrix[:, 0], axis=1)+log_phi[0], axis=0)[1:-1]\n",
    "#     for i in range(2, N+1):\n",
    "#         forward_matrix[1:-1, i] = logsumexp(np.expand_dims(forward_matrix[1:-1, i-1], axis=1)+log_phi[i-1][1:-1][1:-1], axis=0)\n",
    "#     forward_matrix[-1, -1] = logsumexp(np.expand_dims(forward_matrix[1:-1, i-1], axis=1)+log_phi[i-1][1:-1][-1:], axis=0)\n",
    "#     return forward_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_new(log_phi):\n",
    "#     \"\"\"\n",
    "#     forward_matrix of shape (T, N+2), forward_matrix[-1, -1] is log_Z\n",
    "#     \"\"\"\n",
    "#     N, T = log_phi.shape[0]-1, log_phi.shape[1]\n",
    "#     forward_matrix = np.ones((T, N+2))*np.NINF\n",
    "#     forward_matrix[0][0] = 0\n",
    "#     for i in range(1, N+1):\n",
    "#         forward_matrix[1:-1, i] = logsumexp(np.expand_dims(forward_matrix[:, i-1], axis=1)+log_phi[i-1], axis=0)[1:-1]\n",
    "#     forward_matrix[-1, -1] = logsumexp(np.expand_dims(forward_matrix[:, -2], axis=1)+log_phi[-2], axis=0)[-1]\n",
    "#     return forward_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(log_phi):\n",
    "    \"\"\"\n",
    "    backward_matrix of shape (T, N+2), backward_matrix[0, 0] is log_Z\n",
    "    \"\"\"\n",
    "    N, T = log_phi.shape[0]-1, log_phi.shape[1]\n",
    "    backward_matrix = np.ones((T, N+2))*np.NINF\n",
    "    backward_matrix[-1][-1] = 0\n",
    "    for i in range(N, -1, -1):\n",
    "        backward_matrix[:, i] = logsumexp(log_phi[i]+np.expand_dims(backward_matrix[:, i+1], axis=0), axis=1)\n",
    "        if i > 0:\n",
    "            backward_matrix[[0, -1], i] = np.NINF\n",
    "    return backward_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def backward_new(log_phi):\n",
    "#     \"\"\"\n",
    "#     backward_matrix of shape (T, N+2), backward_matrix[0, 0] is log_Z\n",
    "#     \"\"\"\n",
    "#     N, T = log_phi.shape[0]-1, log_phi.shape[1]\n",
    "#     backward_matrix = np.ones((T, N+2))*np.NINF\n",
    "#     backward_matrix[-1][-1] = 0\n",
    "#     for i in range(N, 0, -1):\n",
    "#         backward_matrix[1:-1, i] = logsumexp(log_phi[i]+np.expand_dims(backward_matrix[:, i+1], axis=0), axis=1)[1:-1]\n",
    "#     backward_matrix[0, 0] = logsumexp(log_phi[0]+np.expand_dims(backward_matrix[:, 1], axis=0), axis=1)[0]\n",
    "#     return backward_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88.83919398772295, 88.83919398772295)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "theta = np.random.randn(len(feature2index))\n",
    "feature_activation_map = feature_activation(train_X[0], tag2index, feature2index)\n",
    "log_phi = np.dot(feature_activation_map, theta)\n",
    "\n",
    "forward_matrix = forward(log_phi)\n",
    "backward_matrix = backward(log_phi)\n",
    "\n",
    "forward_matrix[-1, -1], backward_matrix[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88.59832225452645, 88.83919398772295)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "theta = np.random.randn(len(feature2index))\n",
    "feature_activation_map = feature_activation(train_X[0], tag2index, feature2index)\n",
    "log_phi = np.dot(feature_activation_map, theta)\n",
    "\n",
    "forward_matrix = forward_new(log_phi)\n",
    "backward_matrix = backward_new(log_phi)\n",
    "\n",
    "forward_matrix[-1, -1], backward_matrix[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(theta, feature2index, X, Y, tag2index, param=0.1):\n",
    "    log_likelihood = 0\n",
    "    gradient = np.zeros(len(theta))\n",
    "    exp_features = np.zeros(len(theta))\n",
    "    emp_features = np.zeros(len(theta))\n",
    "    T = len(tag2index)\n",
    "    counter = 1\n",
    "\n",
    "    for x, y in zip(X, Y):\n",
    "        N = len(x)\n",
    "\n",
    "        activation_map = feature_activation(x, tag2index, feature2index)\n",
    "        log_phi = np.dot(activation_map, theta)\n",
    "\n",
    "        alpha = forward(log_phi)\n",
    "        beta = backward(log_phi)\n",
    "\n",
    "        log_Z = beta[0][0]\n",
    "\n",
    "        log_likelihood += log_phi[0, 0, y[0]]\n",
    "        emp_features += activation_map[0, 0, y[0]]\n",
    "\n",
    "        for y2 in range(1, T-1): \n",
    "            exp_features += np.exp(alpha[0, 0] + log_phi[0, 0, y2] + beta[y2, 1] - log_Z) *\\\n",
    "                            activation_map[0, 0, y2]\n",
    "\n",
    "        for i in range(1, N):\n",
    "            emp_features += activation_map[i, y[i-1], y[i]]\n",
    "            log_likelihood += log_phi[i, y[i-1], y[i]]\n",
    "\n",
    "            for y1, y2 in itertools.product(range(1, T-1), range(1, T-1)):\n",
    "                exp_features += np.exp(alpha[y1, i] + log_phi[i, y1, y2] + beta[y2, i+1] - log_Z) *\\\n",
    "                                activation_map[i, y1, y2]\n",
    "\n",
    "        for y1 in range(1, T-1):\n",
    "            exp_features += np.exp(alpha[-1, N] + log_phi[N, y1, -1] + beta[-1, N+1] - log_Z) *\\\n",
    "                            activation_map[N, y1, -1]\n",
    "\n",
    "        emp_features += activation_map[N, y[-1], -1]\n",
    "\n",
    "        log_likelihood += log_phi[N, y[-1], -1]\n",
    "\n",
    "        log_likelihood -= log_Z\n",
    "        \n",
    "        print('done with the {}th instance'.format(counter))\n",
    "        counter += 1\n",
    "        \n",
    "    gradient = exp_features - emp_features\n",
    "    \n",
    "    loss = -log_likelihood + param*(np.sum(theta**2))\n",
    "    \n",
    "    gradient += param*2*theta\n",
    "    \n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(X, tag2index, feature2index, theta):\n",
    "    \n",
    "    Y = []\n",
    "    index2tag = {v-1: k for k, v in tag2index.items()}\n",
    "\n",
    "    for x in X:\n",
    "        activation_map = feature_activation(x, tag2index, feature2index)\n",
    "        log_phi = np.dot(activation_map, theta)\n",
    "        N, T = len(x), len(tag2index)\n",
    "        score = np.zeros((T-2, N), dtype=np.float)\n",
    "        path = np.zeros((T-2, N), dtype=np.int)\n",
    "        \n",
    "        score[:, 0] = log_phi[0, 0, 1:-1]\n",
    "        for i in range(1, N):\n",
    "            cross = score[:, i-1][:, None] + log_phi[i, 1:-1, 1:-1]\n",
    "            score[:, i] = np.max(cross, axis=0)\n",
    "            path[:, i] = np.argmax(cross, axis=0)\n",
    "\n",
    "        y = []\n",
    "        last_tag = np.argmax(score[:, -1] + log_phi[-1, 1:-1, -1])\n",
    "        ybest = last_tag\n",
    "\n",
    "        for i in range(len(x)-1,-1,-1):\n",
    "            y.insert(0, ybest)\n",
    "            ybest = path[i, ybest]\n",
    "\n",
    "        Y.append([index2tag[idx] for idx in y])\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "Loss:12.1793\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "Loss:7.0047\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "Loss:6.3043\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "Loss:5.7807\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "Loss:5.4617\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "done with the 1th instance\n",
      "done with the 2th instance\n",
      "Loss:5.4617\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "def callbackF(w):\n",
    "    loss = get_loss_grad(w)[0]\n",
    "    print('Loss:{0:.4f}'.format(loss))\n",
    "    \n",
    "def get_loss_grad(w):\n",
    "    loss, grads = NLL(w, feature2index, train_X[:2], train_Y[:2], tag2index)\n",
    "    return loss, grads\n",
    "\n",
    "init_w = np.zeros(len(feature2index))\n",
    "result = fmin_l_bfgs_b(get_loss_grad, init_w, pgtol=0.01, callback=callbackF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
